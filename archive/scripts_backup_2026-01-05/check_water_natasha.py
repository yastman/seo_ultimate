#!/usr/bin/env python3
"""
Stage 8.1: Water and Nausea Calculator using NATASHA (from GitHub)

SEO 2025 v7.3 (Shop Mode - Buying Guides)

–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–µ–∫—Å—Ç–∞ –ø–æ —Ñ–æ—Ä–º—É–ª–∞–º –ê–¥–≤–µ–≥–æ:
- –í–æ–¥–∞ (%) = (—Å—Ç–æ–ø-—Å–ª–æ–≤–∞ / –≤—Å–µ–≥–æ —Å–ª–æ–≤) √ó 100 | Target: 40-60% (Tier A/B), 40-65% (Tier C)
- –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∞—è —Ç–æ—à–Ω–æ—Ç–∞ = ‚àö(max_lemma_frequency) | Target: ‚â§3.5 (BLOCKER >4.0)
- –ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∞—è —Ç–æ—à–Ω–æ—Ç–∞ = (max_freq / total_significant) √ó 100 | Target: 7-9.5%

Usage:
    python3 scripts/check_water_natasha.py <file.md> [target_min] [target_max]
"""

import math
import re
import sys
from collections import Counter
from pathlib import Path

if __name__ == "__main__" and __package__ is None:
    sys.path.insert(0, str(Path(__file__).resolve().parent.parent))

try:
    from natasha import Doc, MorphVocab, NewsEmbedding, NewsMorphTagger, Segmenter

    NATASHA_FULL = True
except ImportError:
    try:
        from natasha import Doc, MorphVocab, Segmenter

        NATASHA_FULL = False
    except ImportError as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞: {e}")
        print("\n–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:")
        print("  pip install natasha")
        sys.exit(1)

# –ö—ç—à –¥–ª—è —Ç—è–∂—ë–ª—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ Natasha (–∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è –æ–¥–∏–Ω —Ä–∞–∑)
_NLP_CACHE = {}


def get_nlp_pipeline():
    """Singleton –¥–ª—è NLP –æ–±—ä–µ–∫—Ç–æ–≤ Natasha (—ç–∫–æ–Ω–æ–º–∏—Ç ~50MB RAM –ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –≤—ã–∑–æ–≤–∞—Ö)."""
    if "initialized" not in _NLP_CACHE:
        _NLP_CACHE["segmenter"] = Segmenter()
        _NLP_CACHE["morph_vocab"] = MorphVocab()
        if NATASHA_FULL:
            emb = NewsEmbedding()
            _NLP_CACHE["morph_tagger"] = NewsMorphTagger(emb)
        else:
            _NLP_CACHE["morph_tagger"] = None
        _NLP_CACHE["initialized"] = True
    return _NLP_CACHE


# –ó–∞–≥—Ä—É–∑–∏—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤ (stopwords-iso)
def load_stopwords(lang="ru"):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤ –∏–∑ stopwords-{lang}.txt
    Fallback: –µ—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –±–∞–∑–æ–≤—ã–π —Å–ø–∏—Å–æ–∫

    Args:
        lang: 'ru' or 'uk'
    """
    stopwords_file = Path(__file__).parent.parent / "data" / "stopwords" / f"stopwords-{lang}.txt"

    # Fallback –¥–ª—è UK –Ω–∞ RU, –µ—Å–ª–∏ UK —Ñ–∞–π–ª –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç
    if lang == "uk" and not stopwords_file.exists():
        stopwords_file = (
            Path(__file__).parent.parent / "uk" / "data" / "stopwords" / "stopwords-uk.txt"
        )

    if stopwords_file.exists():
        with open(stopwords_file, encoding="utf-8") as f:
            stopwords = {line.strip() for line in f if line.strip()}
        print(f"‚ÑπÔ∏è  –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(stopwords)} —Å—Ç–æ–ø-—Å–ª–æ–≤ –∏–∑ {stopwords_file.name}")
        return stopwords
    else:
        # Fallback: –±–∞–∑–æ–≤—ã–π –Ω–∞–±–æ—Ä —Å—Ç–æ–ø-—Å–ª–æ–≤
        print(f"‚ö†Ô∏è  –§–∞–π–ª {stopwords_file} –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É—é –±–∞–∑–æ–≤—ã–π –Ω–∞–±–æ—Ä")
        try:
            from stop_words import get_stop_words

            lang_code = "russian" if lang == "ru" else "ukrainian"
            return set(get_stop_words(lang_code))
        except ImportError:
            # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä –µ—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ
            if lang == "uk":
                return {
                    "—ñ",
                    "–≤",
                    "–Ω–µ",
                    "–Ω–∞",
                    "–∑",
                    "—â–æ",
                    "—Ü–µ",
                    "—è–∫",
                    "–∞",
                    "–ø–æ",
                    "–¥–ª—è",
                    "–≤—ñ–¥",
                    "—ñ–∑",
                    "–¥–æ",
                    "–∞–±–æ",
                    "–∂–µ",
                    "–ø—Ä–æ",
                    "–∞–ª–µ",
                    "–∑–∞",
                    "—É",
                    "—è–∫–∏–π",
                    "—Ç–∞–∫–æ–∂",
                    "–±—ñ–ª—å—à–µ",
                    "–±—É—Ç–∏",
                    "–º–æ–∂–µ",
                    "–¥—É–∂–µ",
                }
            else:
                return {
                    "–∏",
                    "–≤",
                    "–Ω–µ",
                    "–Ω–∞",
                    "—Å",
                    "—á—Ç–æ",
                    "—ç—Ç–æ",
                    "–∫–∞–∫",
                    "–∞",
                    "–ø–æ",
                    "–¥–ª—è",
                    "–æ—Ç",
                    "–∏–∑",
                    "–∫",
                    "–∏–ª–∏",
                    "–∂–µ",
                    "–æ",
                    "–Ω–æ",
                    "–∑–∞",
                    "—É",
                    "–∫–æ—Ç–æ—Ä—ã–π",
                    "—Ç–∞–∫–∂–µ",
                    "–±–æ–ª–µ–µ",
                    "–±—ã—Ç—å",
                    "–º–æ–∂–µ—Ç",
                    "–æ—á–µ–Ω—å",
                }


# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ (–≤–æ–¥–∞) –∏–∑ –∞–Ω–∞–ª–∏–∑–∞ –ê–¥–≤–µ–≥–æ
ADDITIONAL_STOP_WORDS = {
    "–æ—Å–æ–±–µ–Ω–Ω–æ",
    "–∞–∫—Ç—É–∞–ª—å–Ω–æ",
    "–≤–∞–∂–Ω–æ",
    "–ø–æ–º–Ω–∏—Ç—å",
    "–ø–æ–¥—Ö–æ–¥",
    "—Å–Ω–∏–∂–∞–µ—Ç",
    "—Ä–∏—Å–∫",
    "–ø–æ–∑–≤–æ–ª—è–µ—Ç",
    "—è–≤–ª—è–µ—Ç—Å—è",
    "–¥–∞–Ω–Ω—ã–π",
    "—Å–ª–µ–¥—É–µ—Ç",
    "–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ",
    "–º–æ–∂–Ω–æ",
    "–Ω—É–∂–Ω–æ",
    "—Ä–∞–∑–ª–∏—á–Ω—ã–π",
    "–∫–æ—Ç–æ—Ä—ã–π",
    "—Ç–∞–∫–æ–π",
    "—ç—Ç–æ—Ç",
    "—Å–≤–æ–π",
    "–≤–µ—Å—å",
    "–æ–¥–∏–Ω",
    "–¥—Ä—É–≥–æ–π",
    "–Ω–∞—à",
    "–≤–∞—à",
    "—Ç–æ—Ç",
    "—Å–∞–º",
    "–º–æ—á—å",
    "—Å–∫–∞–∑–∞—Ç—å",
}


def clean_markdown(text):
    """
    –£–¥–∞–ª—è–µ—Ç markdown-—Ä–∞–∑–º–µ—Ç–∫—É.

    NOTE: Uses unified clean_markdown from seo_utils (SSOT).
    Fallback to local implementation if import fails.
    """
    # Try importing from seo_utils (SSOT - Single Source of Truth)
    try:
        from scripts.seo_utils import clean_markdown as seo_clean_markdown

        return seo_clean_markdown(text)
    except ImportError:
        try:
            from seo_utils import clean_markdown as seo_clean_markdown

            return seo_clean_markdown(text)
        except ImportError:
            # Fallback (local implementation if seo_utils unavailable)
            text = re.sub(r"^---\n.*?\n---\n", "", text, flags=re.DOTALL)
            text = re.sub(r"```.*?```", " ", text, flags=re.DOTALL)
            text = re.sub(r"`[^`]+`", " ", text)
            text = re.sub(r"\[([^\]]+)\]\([^\)]+\)", r"\1", text)
            text = re.sub(r"^#{1,6}\s+", "", text, flags=re.MULTILINE)
            text = re.sub(r"\|.*?\|", " ", text)
            text = re.sub(r"[*_]{1,2}([^*_]+)[*_]{1,2}", r"\1", text)
            text = re.sub(r"\s+", " ", text).strip()
            return text


def calculate_metrics_from_text(text: str, lang: str = "ru") -> dict:
    """
    –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–µ–∫—Å—Ç–∞ (–≤–æ–¥–∞, —Ç–æ—à–Ω–æ—Ç–∞).

    Args:
        text: Markdown —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        lang: 'ru' or 'uk' - —è–∑—ã–∫ –¥–ª—è —Å—Ç–æ–ø-—Å–ª–æ–≤

    Returns:
        Dict —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∏–ª–∏ None –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç –ø—É—Å—Ç–æ–π

    NOTE: –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π pipeline Natasha:
        1. doc.segment(segmenter)
        2. doc.tag_morph(morph_tagger)  # –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—è
        3. token.lemmatize(morph_vocab)  # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ –º–µ—Ç–æ–¥ —Ç–æ–∫–µ–Ω–∞
    """
    # –ó–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –¥–ª—è –Ω—É–∂–Ω–æ–≥–æ —è–∑—ã–∫–∞
    stopwords = load_stopwords(lang)
    stopwords.update(ADDITIONAL_STOP_WORDS)  # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ

    # –ü–æ–ª—É—á–∞–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã NLP
    nlp = get_nlp_pipeline()
    segmenter = nlp["segmenter"]
    morph_vocab = nlp["morph_vocab"]
    morph_tagger = nlp["morph_tagger"]

    # –û—á–∏—Å—Ç–∏—Ç—å –æ—Ç markdown
    clean_text = clean_markdown(text)

    doc = Doc(clean_text)
    doc.segment(segmenter)

    # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω –ø–æ–ª–Ω—ã–π Natasha)
    if morph_tagger:
        doc.tag_morph(morph_tagger)

    if not doc.tokens:
        print("‚ùå –¢–µ–∫—Å—Ç –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤")
        return None

    # –ò–∑–≤–ª–µ—á—å —Ç–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏–µ —Å–ª–æ–≤–∞
    russian_tokens = [
        token for token in doc.tokens if re.match(r"[–∞-—è—ë]+", token.text.lower(), re.UNICODE)
    ]

    if not russian_tokens:
        print("‚ùå –¢–µ–∫—Å—Ç –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ä—É—Å—Å–∫–∏—Ö —Å–ª–æ–≤")
        return None

    total_words = len(russian_tokens)
    words_lower = [token.text.lower() for token in russian_tokens]

    # 1. –í–û–î–ê: —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
    # –ö–∞–ª–∏–±—Ä–æ–≤–æ—á–Ω—ã–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–ª—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –ê–¥–≤–µ–≥–æ (Natasha ~22% ‚Üí –ê–¥–≤–µ–≥–æ ~52%)
    ADVEGO_MULTIPLIER = 2.4  # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ —Å –ê–¥–≤–µ–≥–æ (v7.2 Update)

    water_count = sum(1 for word in words_lower if word in stopwords)
    water_percent_raw = (water_count / total_words) * 100
    water_percent = water_percent_raw * ADVEGO_MULTIPLIER

    # 2. –õ–ï–ú–ú–ê–¢–ò–ó–ê–¶–ò–Ø (–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Å–ø–æ—Å–æ–± —á–µ—Ä–µ–∑ Natasha)
    lemmas = []
    for token in russian_tokens:
        if morph_tagger and hasattr(token, "pos"):
            # –ü–æ–ª–Ω—ã–π pipeline: –∏—Å–ø–æ–ª—å–∑—É–µ–º lemmatize() —á–µ—Ä–µ–∑ –º–µ—Ç–æ–¥ —Ç–æ–∫–µ–Ω–∞
            token.lemmatize(morph_vocab)
            if token.lemma:
                lemmas.append(token.lemma)
            else:
                lemmas.append(token.text.lower())
        else:
            # Fallback: –ø—Ä—è–º–æ–π –≤—ã–∑–æ–≤ morph_vocab (—Å—Ç–∞—Ä—ã–π —Å–ø–æ—Å–æ–±)
            parsed = morph_vocab(token.text.lower())
            if parsed:
                lemmas.append(parsed[0].normal)
            else:
                lemmas.append(token.text.lower())

    # 3. –ö–õ–ê–°–°–ò–ß–ï–°–ö–ê–Ø –¢–û–®–ù–û–¢–ê
    lemma_counts = Counter(lemmas)
    significant_lemma_counts = {
        lemma: count for lemma, count in lemma_counts.items() if lemma not in stopwords
    }

    if significant_lemma_counts:
        most_common_lemma, max_frequency = max(significant_lemma_counts.items(), key=lambda x: x[1])
    else:
        most_common_lemma, max_frequency = lemma_counts.most_common(1)[0]

    classic_nausea = math.sqrt(max_frequency)

    # 4. –ê–ö–ê–î–ï–ú–ò–ß–ï–°–ö–ê–Ø –¢–û–®–ù–û–¢–ê (Advego-like)
    significant_lemmas = {
        lemma: count
        for lemma, count in lemma_counts.items()
        if count > 1 and lemma not in stopwords
    }

    if significant_lemmas:
        total_significant = sum(significant_lemmas.values())
        max_freq_significant = max(significant_lemmas.values())
        most_common_significant = max(significant_lemmas, key=significant_lemmas.get)
        academic_nausea = max_freq_significant / total_significant * 100
    else:
        total_significant = 0
        max_freq_significant = 0
        most_common_significant = None
        academic_nausea = 0.0

    # 5. –ò–ù–î–ï–ö–° –ü–û–í–¢–û–†–û–í –õ–ï–ú–ú
    repeated_words_count = sum(count for count in lemma_counts.values() if count > 1)
    lemma_repetition_index = (repeated_words_count / total_words) * 100 if total_words > 0 else 0.0

    return {
        "total_words": total_words,
        "water_count": water_count,
        "water_percent_raw": water_percent_raw,
        "water_percent": water_percent,
        "classic_nausea": classic_nausea,
        "most_common_lemma": most_common_lemma,
        "max_frequency": max_frequency,
        "academic_nausea": academic_nausea,
        "most_common_significant": most_common_significant,
        "max_freq_significant": max_freq_significant,
        "total_significant": total_significant,
        "lemma_repetition_index": lemma_repetition_index,
        "repeated_words_count": repeated_words_count,
        "unique_lemmas": len(lemma_counts),
    }


def calculate_metrics(file_path):
    with open(file_path, encoding="utf-8") as f:
        text = f.read()
    return calculate_metrics_from_text(text)


def check_water(file_path, target_min=40, target_max=60):
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–æ–¥—É –∏ —Ç–æ—à–Ω–æ—Ç—É –ø–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º SEO 2025 v7.3.

    Targets:
    - –í–æ–¥–∞: 40-60% (Tier A/B), 40-65% (Tier C)
    - –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∞—è —Ç–æ—à–Ω–æ—Ç–∞: ‚â§3.5
    - –ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∞—è —Ç–æ—à–Ω–æ—Ç–∞: 7-9.5%
    """
    print(f"üìä –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–µ–∫—Å—Ç–∞ (NATASHA): {Path(file_path).name}\n")

    metrics = calculate_metrics(file_path)

    if not metrics:
        return 1

    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print(f"–í—Å–µ–≥–æ —Å–ª–æ–≤: {metrics['total_words']}")
    print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ª–µ–º–º: {metrics['unique_lemmas']}")
    print()

    # 1. –í–û–î–ê
    print(f"üíß –í–û–î–ê (–ê–¥–≤–µ–≥–æ): {metrics['water_percent']:.1f}%")
    print(
        f"   Raw (Natasha): {metrics['water_percent_raw']:.1f}% √ó 2.4 = {metrics['water_percent']:.1f}%"
    )
    print(f"   –°—Ç–æ–ø-—Å–ª–æ–≤–∞: {metrics['water_count']} –∏–∑ {metrics['total_words']}")
    print(f"   –¶–µ–ª—å: {target_min}-{target_max}%")

    if target_min <= metrics["water_percent"] <= target_max:
        print("   ‚úÖ PASS")
    elif metrics["water_percent"] > target_max:
        excess = metrics["water_percent"] - target_max
        if excess <= 5.0:
            print(f"   ‚ö†Ô∏è WARNING: –ü—Ä–µ–≤—ã—à–µ–Ω–∏–µ –Ω–∞ {excess:.1f}% (–¥–æ–ø—É—Å—Ç–∏–º–æ –¥–ª—è Tier C)")
        else:
            print(f"   ‚ö†Ô∏è WARNING: –ü—Ä–µ–≤—ã—à–µ–Ω–∏–µ –Ω–∞ {excess:.1f}%")
    else:
        deficit = target_min - metrics["water_percent"]
        print(f"   ‚ö†Ô∏è WARNING: –ù–∏–∂–µ –º–∏–Ω–∏–º—É–º–∞ –Ω–∞ {deficit:.1f}%")

    print()

    # 2. –ö–õ–ê–°–°–ò–ß–ï–°–ö–ê–Ø –¢–û–®–ù–û–¢–ê
    print(f"ü§¢ –ö–õ–ê–°–°–ò–ß–ï–°–ö–ê–Ø –¢–û–®–ù–û–¢–ê: {metrics['classic_nausea']:.2f}")
    print(
        f"   –°–∞–º–æ–µ —á–∞—Å—Ç–æ–µ —Å–ª–æ–≤–æ: '{metrics['most_common_lemma']}' ({metrics['max_frequency']} —Ä–∞–∑)"
    )
    print("   –¶–µ–ª—å: ‚â§3.5 (BLOCKER >4.0)")

    if metrics["classic_nausea"] <= 3.5:
        print("   ‚úÖ PASS")
    elif metrics["classic_nausea"] <= 4.0:
        print(f"   ‚ö†Ô∏è WARNING: –ü—Ä–µ–≤—ã—à–µ–Ω–∏–µ ({metrics['classic_nausea']:.2f} > 3.5)")
    else:
        print(f"   ‚ùå BLOCKER: –ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–µ ({metrics['classic_nausea']:.2f} > 4.0)")

    print()

    # 3. –ê–ö–ê–î–ï–ú–ò–ß–ï–°–ö–ê–Ø –¢–û–®–ù–û–¢–ê
    # v7.3 Target: 7-9.5%
    ACADEMIC_MIN = 7.0
    ACADEMIC_MAX = 9.5

    print(f"üìö –ê–ö–ê–î–ï–ú–ò–ß–ï–°–ö–ê–Ø –¢–û–®–ù–û–¢–ê (Advego-like): {metrics['academic_nausea']:.1f}%")
    if metrics["most_common_significant"]:
        print(
            f"   –°–∞–º–æ–µ —á–∞—Å—Ç–æ–µ –∑–Ω–∞—á–∏–º–æ–µ —Å–ª–æ–≤–æ: '{metrics['most_common_significant']}' ({metrics['max_freq_significant']} —Ä–∞–∑)"
        )
        print(f"   –ó–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤ (–±–µ–∑ —Å—Ç–æ–ø-—Å–ª–æ–≤, freq>1): {metrics['total_significant']}")
    else:
        print("   –ù–µ—Ç –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤")

    print(f"   –¶–µ–ª—å: {ACADEMIC_MIN}-{ACADEMIC_MAX}% (–ê–¥–≤–µ–≥–æ –æ–ø—Ç–∏–º—É–º)")

    if ACADEMIC_MIN <= metrics["academic_nausea"] <= ACADEMIC_MAX:
        print("   ‚úÖ PASS (–ê–¥–≤–µ–≥–æ –æ–ø—Ç–∏–º—É–º)")
    elif metrics["academic_nausea"] < ACADEMIC_MIN:
        print(f'   üü¶ INFO: –¢–µ–∫—Å—Ç "—Å—É—Ö–æ–π" ({metrics["academic_nausea"]:.1f}% < {ACADEMIC_MIN}%)')
    elif ACADEMIC_MAX < metrics["academic_nausea"] <= 12.0:
        print(
            f"   ‚ö†Ô∏è WARNING: –ù–∞—á–∏–Ω–∞–µ—Ç—Å—è –ø–µ—Ä–µ—Å–ø–∞–º ({metrics['academic_nausea']:.1f}% > {ACADEMIC_MAX}%)"
        )
    else:
        print(f"   ‚ùå BLOCKER: –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –ø–µ—Ä–µ—Å–ø–∞–º ({metrics['academic_nausea']:.1f}% > 12%)")

    print()

    # 4. –ò–ù–î–ï–ö–° –ü–û–í–¢–û–†–û–í –õ–ï–ú–ú
    print(f"üîÅ –ò–ù–î–ï–ö–° –ü–û–í–¢–û–†–û–í –õ–ï–ú–ú: {metrics['lemma_repetition_index']:.1f}%")
    print(f"   –ü–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –ª–µ–º–º—ã: {metrics['repeated_words_count']} –∏–∑ {metrics['total_words']}")
    print("   (–≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ –¥–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞)")

    print()
    print("‚ÑπÔ∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ Natasha –¥–ª—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞")
    # –î–ª—è –≤—ã–≤–æ–¥–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–Ω–æ–≤–∞ –∑–∞–≥—Ä—É–∑–∏–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ (–∏–ª–∏ –º–æ–∂–Ω–æ –ø–µ—Ä–µ–¥–∞—Ç—å –∏—Ö, –Ω–æ –ø—Ä–æ—â–µ –∑–∞–≥—Ä—É–∑–∏—Ç—å)
    stopwords_list = load_stopwords()
    print(f"‚ÑπÔ∏è –°—Ç–æ–ø-—Å–ª–æ–≤: {len(stopwords_list)} (stopwords-iso + –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ)")

    return 0


def main(argv: list[str] | None = None) -> int:
    argv = sys.argv[1:] if argv is None else argv

    if len(argv) < 1:
        print("Usage: check_water_natasha.py <file.md> [target_min] [target_max]")
        return 1

    file_path = argv[0]

    # Defaults v7.3: 40-60
    target_min = int(argv[1]) if len(argv) > 1 else 40
    target_max = int(argv[2]) if len(argv) > 2 else 60

    if not Path(file_path).exists():
        print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
        return 1

    return check_water(file_path, target_min, target_max)


if __name__ == "__main__":
    raise SystemExit(main())
