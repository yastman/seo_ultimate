#!/usr/bin/env python3
"""
MEGA URL Aggregation ‚Äî Stage -4 (V3.1 Cluster-First)

–ö–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É–µ—Ç cluster-level URLs –∏–∑ –≤—Å–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –≤ –≥–ª–æ–±–∞–ª—å–Ω—ã–π MEGA –ø—É–ª.

V3.1 –ò–ó–ú–ï–ù–ï–ù–ò–ï:
- –ù–ï –∏–∑–≤–ª–µ–∫–∞–µ—Ç URLs –∏–∑ SERP –Ω–∞–ø—Ä—è–º—É—é
- –í–ú–ï–°–¢–û —ç—Ç–æ–≥–æ –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É–µ—Ç cluster_urls*.txt|csv –∏–∑ categories/*/competitors/
- –ü—Ä–∏–º–µ–Ω—è–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—É—é –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—é
- –§–æ—Ä–º–∏—Ä—É–µ—Ç mega_urls.txt –¥–ª—è –µ–¥–∏–Ω–æ–≥–æ Screaming Frog –∑–∞–ø—É—Å–∫–∞

Workflow:
1. –ù–∞–π—Ç–∏ –≤—Å–µ categories/*/competitors/cluster_urls_raw.txt
2. –ù–∞–π—Ç–∏ –≤—Å–µ categories/*/competitors/cluster_urls.txt
3. –ù–∞–π—Ç–∏ –≤—Å–µ categories/*/competitors/cluster_urls_map.csv
4. –ö–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä–æ–≤–∞—Ç—å —Å –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–µ–π
5. –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ data/mega/

Exit codes:
- 0: OK (‚â•30 URLs extracted)
- 1: Warning (10-29 URLs)
- 2: Fail (<10 URLs or no cluster files found)
"""

from __future__ import annotations

import argparse
import csv as csv_module
from pathlib import Path

# ============================================================================
# FUNCTIONS
# ============================================================================


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="MEGA URL Aggregation (V3.1 Cluster-First)")
    parser.add_argument(
        "--categories-dir",
        type=str,
        default="categories",
        help="Categories directory (default: categories)",
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default="data/mega",
        help="Output directory (default: data/mega)",
    )
    parser.add_argument(
        "--min-urls",
        type=int,
        default=30,
        help="Minimum total URLs required (default: 30)",
    )
    return parser


def resolve_dir(base_dir: Path, path_str: str) -> Path:
    path = Path(path_str)
    if path.is_absolute():
        return path
    return (base_dir / path).resolve()


def find_cluster_files(categories_dir: Path) -> dict[str, dict[str, Path | None]]:
    """
    –ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ cluster files –≤ categories/*/competitors/.

    Returns:
        {
            "aktivnaya-pena": {
                "raw": Path(...),
                "clean": Path(...),
                "map": Path(...)
            },
            ...
        }
    """
    cluster_files: dict[str, dict[str, Path | None]] = {}

    for category_dir in categories_dir.iterdir():
        if not category_dir.is_dir():
            continue

        slug = category_dir.name
        competitors_dir = category_dir / "competitors"

        if not competitors_dir.exists():
            continue

        raw_file = competitors_dir / "cluster_urls_raw.txt"
        clean_file = competitors_dir / "cluster_urls.txt"
        map_file = competitors_dir / "cluster_urls_map.csv"

        # –¢—Ä–µ–±—É–µ–º –º–∏–Ω–∏–º—É–º clean_file (TOP-N –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤)
        if clean_file.exists():
            cluster_files[slug] = {
                "raw": raw_file if raw_file.exists() else None,
                "clean": clean_file,
                "map": map_file if map_file.exists() else None,
            }

    return cluster_files


def read_urls_from_file(file_path: Path) -> list[str]:
    """–ß–∏—Ç–∞–µ—Ç URLs –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞."""
    if not file_path or not file_path.exists():
        return []

    urls = []
    with file_path.open("r", encoding="utf-8") as f:
        for line in f:
            url = line.strip()
            if url and url.startswith("http"):
                urls.append(url)

    return urls


def normalize_url(url: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç URL –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏."""
    url_lower = url.lower()
    # –£–±—Ä–∞—Ç—å trailing slash
    if url_lower.endswith("/"):
        url_lower = url_lower[:-1]
    return url_lower


def aggregate_raw_urls(cluster_files: dict[str, dict[str, Path | None]]) -> list[str]:
    """
    –ö–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É–µ—Ç –≤—Å–µ cluster_urls_raw.txt.

    Returns:
        List of raw URLs (with duplicates, –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏)
    """
    all_raw = []

    for slug, files in cluster_files.items():
        if files["raw"]:
            urls = read_urls_from_file(files["raw"])
            all_raw.extend(urls)
            print(f"   {slug}: {len(urls)} raw URLs")

    return all_raw


def aggregate_clean_urls(cluster_files: dict[str, dict[str, Path | None]]) -> tuple[list[str], int]:
    """
    –ö–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É–µ—Ç –≤—Å–µ cluster_urls.txt —Å –≥–ª–æ–±–∞–ª—å–Ω–æ–π –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–µ–π.

    Returns:
        (unique_urls, duplicates_removed)
    """
    url_map = {}  # normalized -> original

    for slug, files in cluster_files.items():
        urls = read_urls_from_file(files["clean"])

        for url in urls:
            url_normalized = normalize_url(url)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–≤—É—é –≤—Å—Ç—Ä–µ—á–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é URL
            if url_normalized not in url_map:
                url_map[url_normalized] = url

        print(f"   {slug}: {len(urls)} clean URLs")

    unique_urls = list(url_map.values())
    duplicates_removed = sum(len(read_urls_from_file(f["clean"])) for f in cluster_files.values()) - len(unique_urls)

    return unique_urls, duplicates_removed


def aggregate_url_maps(cluster_files: dict[str, dict[str, Path | None]]) -> list[dict]:
    """
    –ö–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É–µ—Ç –≤—Å–µ cluster_urls_map.csv.

    Returns:
        [
            {"slug": "...", "cluster_name": "...", "seed_phrase": "...", ...},
            ...
        ]
    """
    all_mappings = []

    for slug, files in cluster_files.items():
        if not files["map"]:
            continue

        with files["map"].open("r", encoding="utf-8") as f:
            reader = csv_module.DictReader(f)

            for row in reader:
                all_mappings.append(row)

        with files["map"].open("r", encoding="utf-8") as f_count:
            print(f"   {slug}: {len(list(csv_module.DictReader(f_count)))} mappings")

    return all_mappings


def save_mega_files(output_dir: Path, raw_urls: list[str], clean_urls: list[str], url_mappings: list[dict]) -> None:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç MEGA —Ñ–∞–π–ª—ã."""
    # –°–æ–∑–¥–∞—Ç—å output dir
    output_dir.mkdir(parents=True, exist_ok=True)

    mega_urls_raw = output_dir / "mega_urls_raw.txt"
    mega_urls = output_dir / "mega_urls.txt"
    mega_urls_map = output_dir / "mega_urls_map.csv"

    # 1. mega_urls_raw.txt (–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞)
    with mega_urls_raw.open("w", encoding="utf-8") as f:
        for url in raw_urls:
            f.write(f"{url}\n")

    # 2. mega_urls.txt (–¥–ª—è Screaming Frog)
    with mega_urls.open("w", encoding="utf-8") as f:
        for url in clean_urls:
            f.write(f"{url}\n")

    # 3. mega_urls_map.csv (–º–∞–ø–ø–∏–Ω–≥)
    with mega_urls_map.open("w", encoding="utf-8", newline="") as f:
        if url_mappings:
            fieldnames = url_mappings[0].keys()
            writer = csv_module.DictWriter(f, fieldnames=fieldnames)

            writer.writeheader()
            writer.writerows(url_mappings)

    print("\n‚úÖ Saved MEGA files:")
    print(f"   - {mega_urls_raw} ({len(raw_urls)} URLs)")
    print(f"   - {mega_urls} ({len(clean_urls)} URLs)")
    print(f"   - {mega_urls_map} ({len(url_mappings)} mappings)")


# ============================================================================
# MAIN
# ============================================================================


def main(argv: list[str] | None = None) -> int:
    parser = build_parser()
    args = parser.parse_args(argv)

    base_dir = Path(__file__).resolve().parent.parent
    categories_dir = resolve_dir(base_dir, args.categories_dir)
    output_dir = resolve_dir(base_dir, args.output_dir)

    print("\nüéØ MEGA URL Aggregation (V3.1 Cluster-First)")
    print(f"   Categories dir: {categories_dir}")
    print(f"   Output dir: {output_dir}")
    print(f"   Min URLs threshold: {args.min_urls}")

    # 1. –ù–∞–π—Ç–∏ cluster files
    print(f"\nüìÅ Finding cluster files in {categories_dir}...")
    if not categories_dir.exists():
        print(f"\n‚ùå ERROR: Categories directory does not exist: {categories_dir}")
        return 2

    cluster_files = find_cluster_files(categories_dir)

    if not cluster_files:
        print("\n‚ùå ERROR: No cluster files found!")
        print("   Expected: categories/*/competitors/cluster_urls.txt")
        print("   Run cluster_url_selection.py for each category first")
        return 2

    print(f"   ‚úÖ Found cluster files for {len(cluster_files)} categories:")
    for slug in cluster_files:
        print(f"      - {slug}")

    # 2. Aggregate RAW URLs (–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞)
    print("\nüìä Aggregating RAW URLs (cluster_urls_raw.txt)...")
    raw_urls = aggregate_raw_urls(cluster_files)
    print(f"   ‚úÖ Total RAW URLs: {len(raw_urls)}")

    # 3. Aggregate CLEAN URLs (–¥–ª—è Screaming Frog)
    print("\nüîÑ Aggregating CLEAN URLs (cluster_urls.txt) with dedup...")
    clean_urls, duplicates = aggregate_clean_urls(cluster_files)
    print(f"   ‚úÖ Total CLEAN URLs: {len(clean_urls)}")
    print(f"   ‚úÖ Duplicates removed: {duplicates}")

    # 4. Aggregate URL mappings
    print("\nüó∫Ô∏è  Aggregating URL mappings (cluster_urls_map.csv)...")
    url_mappings = aggregate_url_maps(cluster_files)
    print(f"   ‚úÖ Total mappings: {len(url_mappings)}")

    # 5. Save MEGA files
    print("\nüíæ Saving MEGA files...")
    save_mega_files(output_dir, raw_urls, clean_urls, url_mappings)

    # 6. Validation
    print("\nüìä Summary:")
    print(f"   Categories: {len(cluster_files)}")
    print(f"   RAW URLs: {len(raw_urls)}")
    print(f"   CLEAN URLs (deduped): {len(clean_urls)}")
    print(f"   URL mappings: {len(url_mappings)}")
    if len(raw_urls) > 0:
        print(f"   Removal rate: {(len(raw_urls) - len(clean_urls)) / len(raw_urls) * 100:.1f}%")

    # Exit code
    if len(clean_urls) < 10:
        print(f"\n‚ùå FAIL: Only {len(clean_urls)} URLs (<10 minimum)")
        return 2
    elif len(clean_urls) < args.min_urls:
        print(f"\n‚ö†Ô∏è  WARNING: Only {len(clean_urls)} URLs (<{args.min_urls} target)")
        return 1
    else:
        print(f"\n‚úÖ SUCCESS: {len(clean_urls)} URLs extracted (‚â•{args.min_urls})")
        return 0


if __name__ == "__main__":
    raise SystemExit(main())
