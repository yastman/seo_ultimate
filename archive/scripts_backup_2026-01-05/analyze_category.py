#!/usr/bin/env python3
"""
analyze_category.py ‚Äî Category Analysis for LLM (Google 2025 Approach)

–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∏ –≤—ã–¥–∞—ë—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è LLM.
–ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ—Ç tier-based –ø–æ–¥—Ö–æ–¥. –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ ‚Äî –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑.

D+E Fallback Pattern:
1. –°–Ω–∞—á–∞–ª–∞ –∏—â–µ—Ç _clean.json (12 clustered keywords)
2. –ó–∞—Ç–µ–º –∏—â–µ—Ç raw .json (52 keywords)
3. Fallback –Ω–∞ CSV –µ—Å–ª–∏ JSON –Ω–µ—Ç

–ü—Ä–∏–Ω—Ü–∏–ø (Google December 2025):
- Depth over length
- Editorial content = buying guide
- Intent matching > keyword density
- Relative usefulness

Output:
- JSON —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏ –¥–ª—è LLM
- LLM —Ä–µ—à–∞–µ—Ç –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –≥–ª—É–±–∏–Ω—ã

Usage:
    python3 scripts/analyze_category.py <slug>
    python3 scripts/analyze_category.py <slug> --json
    python3 scripts/analyze_category.py --list
"""

import csv
import json
import sys
from datetime import UTC, datetime
from pathlib import Path

# =============================================================================
# Configuration
# =============================================================================

SCRIPT_DIR = Path(__file__).parent
PROJECT_ROOT = SCRIPT_DIR.parent
SEMANTICS_CSV = PROJECT_ROOT / "–°—Ç—Ä—É–∫—Ç—É—Ä–∞ _Ultimate.csv"
CATEGORIES_DIR = PROJECT_ROOT / "categories"


# Language-specific paths
def get_categories_dir(lang: str = "ru") -> Path:
    """Get categories directory based on language."""
    if lang == "uk":
        return PROJECT_ROOT / "uk" / "categories"
    return PROJECT_ROOT / "categories"


# Current language (set by main)
CURRENT_LANG = "ru"

# =============================================================================
# D+E Fallback: Load keywords from best available source
# =============================================================================


def load_keywords_for_slug(slug: str, lang: str = "ru") -> tuple[list[dict], str, dict | None]:
    """
    D+E Fallback Pattern: Load keywords from best available source.

    Priority:
    1. {slug}_clean.json ‚Äî clustered (12 keywords)
    2. {slug}.json ‚Äî raw parsed (52 keywords)
    3. CSV fallback

    Args:
        slug: Category slug
        lang: Language code ("ru" or "uk")

    Returns:
        Tuple[List[Dict], str, Optional[Dict]]:
            - keywords list (normalized to {keyword, volume})
            - source: "clean_json", "raw_json", or "csv"
            - extra_data: seo_titles, entity_dictionary, content_rules (from clean.json)
    """
    categories_dir = get_categories_dir(lang)
    category_dir = categories_dir / slug / "data"

    # 1. Try _clean.json first (preferred)
    clean_json = category_dir / f"{slug}_clean.json"
    if clean_json.exists():
        with open(clean_json, encoding="utf-8") as f:
            data = json.load(f)

        # Extract keywords from clean format (clustered).
        # Clean JSON in this project includes 12 keywords total:
        # primary + secondary + supporting + commercial (meta-only).
        keywords = []
        for category in ["primary", "secondary", "supporting", "commercial"]:
            for kw in data.get("keywords", {}).get(category, []):
                # Commercial keywords are meta-only by convention.
                use_in = kw.get("use_in")
                target = 0 if use_in == "meta_only" else kw.get("target", 1)
                keywords.append(
                    {
                        "keyword": kw["keyword"],
                        "volume": kw.get("volume", 0),
                        "cluster": kw.get("cluster", "unknown"),
                        "target": target,
                    }
                )

        extra_data = {
            "seo_titles": data.get("seo_titles"),
            "entity_dictionary": data.get("entity_dictionary"),
            "content_rules": data.get("content_rules"),
            "stats": data.get("stats"),
        }

        return keywords, "clean_json", extra_data

    # 2. Try raw .json
    raw_json = category_dir / f"{slug}.json"
    if raw_json.exists():
        with open(raw_json, encoding="utf-8") as f:
            data = json.load(f)

        # Extract keywords from raw format
        keywords = []
        for category in ["primary", "secondary", "supporting"]:
            for kw in data.get("keywords", {}).get(category, []):
                keywords.append({"keyword": kw["keyword"], "volume": kw.get("volume", 0)})

        return keywords, "raw_json", None

    # 3. Fallback to CSV
    l3_name = SLUG_TO_L3.get(slug)
    if not l3_name:
        return [], "csv", None

    categories = read_semantics_csv(str(SEMANTICS_CSV))
    keywords = categories.get(l3_name, [])

    return keywords, "csv", None


# L3 name to slug mapping - imported from seo_utils.py (SSOT)
try:
    from scripts.seo_utils import L3_TO_SLUG, SLUG_TO_L3, get_commercial_modifiers, get_l3_slug
except ImportError:
    try:
        from seo_utils import L3_TO_SLUG, SLUG_TO_L3, get_commercial_modifiers, get_l3_slug
    except ImportError:
        # Fallback for standalone execution
        L3_TO_SLUG = {
            "–ê–∫—Ç–∏–≤–Ω–∞—è –ø–µ–Ω–∞": "aktivnaya-pena",
            "–î–ª—è —Ä—É—á–Ω–æ–π –º–æ–π–∫–∏": "dlya-ruchnoy-moyki",
            "–û—á–∏—Å—Ç–∏—Ç–µ–ª–∏ —Å—Ç–µ–∫–æ–ª": "ochistiteli-stekol",
            "–ì–ª–∏–Ω–∞ –∏ –∞–≤—Ç–æ—Å–∫—Ä–∞–±—ã": "glina-i-avtoskraby",
            "–ê–Ω—Ç–∏–º–æ—à–∫–∞": "antimoshka",
            "–ê–Ω—Ç–∏–±–∏—Ç—É–º": "antibitum",
            "–ß–µ—Ä–Ω–∏—Ç–µ–ª–∏ —à–∏–Ω": "cherniteli-shin",
            "–û—á–∏—Å—Ç–∏—Ç–µ–ª–∏ –¥–∏—Å–∫–æ–≤": "ochistiteli-diskov",
            "–û—á–∏—Å—Ç–∏—Ç–µ–ª–∏ —à–∏–Ω": "ochistiteli-shin",
            "–ê–ø–ø–∞—Ä–∞—Ç—ã Tornador": "apparaty-tornador",
            "–ú–µ—Ö–æ–≤—ã–µ": "mekhovye",
            "–ü–æ—Ä–æ–ª–æ–Ω–æ–≤—ã–µ": "porolonovye",
            "–ù–∞–±–æ—Ä—ã –¥–ª—è –º–æ–π–∫–∏": "nabory-dlya-moyki",
            "–ù–∞–±–æ—Ä—ã –¥–ª—è –ø–æ–ª–∏—Ä–æ–≤–∫–∏": "nabory-dlya-polirovki",
            "–ù–∞–±–æ—Ä—ã –¥–ª—è —É—Ö–æ–¥–∞ –∑–∞ –∫–æ–∂–µ–π": "nabory-dlya-ukhoda-za-kozhey",
            "–ù–∞–±–æ—Ä—ã –¥–ª—è —Ö–∏–º—á–∏—Å—Ç–∫–∏": "nabory-dlya-khimchistki",
            "–ü–æ–¥–∞—Ä–æ—á–Ω—ã–µ –Ω–∞–±–æ—Ä—ã": "podarochnye-nabory",
        }
        SLUG_TO_L3 = {v: k for k, v in L3_TO_SLUG.items()}

        def get_l3_slug(name):
            return L3_TO_SLUG.get(name, name.lower().replace(" ", "-"))

        _COMMERCIAL_MODIFIERS_FALLBACK = [
            "–∫—É–ø–∏—Ç—å",
            "—Ü–µ–Ω–∞",
            "—Å—Ç–æ–∏–º–æ—Å—Ç—å",
            "–∑–∞–∫–∞–∑–∞—Ç—å",
            "–≤ –Ω–∞–ª–∏—á–∏–∏",
            "–¥–æ—Å—Ç–∞–≤–∫–∞",
            "–Ω–µ–¥–æ—Ä–æ–≥–æ",
            "–¥—ë—à–µ–≤–æ",
            "–¥–µ—à–µ–≤–æ",
            "–∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞–≥–∞–∑–∏–Ω",
            "–º–∞–≥–∞–∑–∏–Ω",
            "–∫–∞—Ç–∞–ª–æ–≥",
        ]
        _COMMERCIAL_MODIFIERS_UK_FALLBACK = [
            "–∫—É–ø–∏—Ç–∏",
            "—Ü—ñ–Ω–∞",
            "–≤–∞—Ä—Ç—ñ—Å—Ç—å",
            "–∑–∞–º–æ–≤–∏—Ç–∏",
            "–≤ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ",
            "–¥–æ—Å—Ç–∞–≤–∫–∞",
            "–Ω–µ–¥–æ—Ä–æ–≥–æ",
            "–¥–µ—à–µ–≤–æ",
            "—ñ–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞–≥–∞–∑–∏–Ω",
            "–º–∞–≥–∞–∑–∏–Ω",
            "–∫–∞—Ç–∞–ª–æ–≥",
        ]

        def get_commercial_modifiers(lang="ru"):
            return _COMMERCIAL_MODIFIERS_UK_FALLBACK if lang == "uk" else _COMMERCIAL_MODIFIERS_FALLBACK


# Backwards-compatible exports for tests/legacy imports.
COMMERCIAL_MODIFIERS = get_commercial_modifiers("ru")
COMMERCIAL_MODIFIERS_UK = get_commercial_modifiers("uk")

# Coverage targets (SSOT)
try:
    from scripts.config import get_guidelines_coverage_target
except ImportError:
    from config import get_guidelines_coverage_target


# =============================================================================
# CSV Parser
# =============================================================================


def read_semantics_csv(csv_path: str) -> dict[str, list[dict]]:
    """
    –ß–∏—Ç–∞–µ—Ç CSV —Å–µ–º–∞–Ω—Ç–∏–∫–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç keywords –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º L3.
    """
    categories: dict[str, list[dict]] = {}
    current_l3: str | None = None

    with open(csv_path, encoding="utf-8") as f:
        reader = csv.reader(f)
        for row in reader:
            if not row or all((not cell) or (not cell.strip()) for cell in row):
                continue

            if not row[0] or not row[0].strip():
                continue

            phrase = row[0].strip()
            volume_str = row[2].strip() if len(row) > 2 else ""

            # Detect L3 category
            if phrase.startswith("L3:"):
                current_l3 = phrase.replace("L3:", "").strip()
                if current_l3 not in categories:
                    categories[current_l3] = []
                continue

            # Skip L1, L2 headers
            if phrase.startswith("L1:") or phrase.startswith("L2:"):
                current_l3 = None
                continue

            # Skip SEO filter headers
            if phrase.startswith("SEO-–§–∏–ª—å—Ç—Ä:"):
                continue

            # Skip boundary rows
            count_str = row[1].strip() if len(row) > 1 else ""
            if current_l3 and phrase.lower() == "–∫–∞—Ç–µ–≥–æ—Ä–∏—è" and count_str.isdigit() and not volume_str:
                current_l3 = None
                continue

            if not current_l3:
                continue

            # Skip subsection headers
            if count_str and not volume_str:
                continue

            # Regular keyword requires numeric volume
            if not volume_str.isdigit():
                continue

            volume = int(volume_str)
            categories[current_l3].append({"keyword": phrase, "volume": volume})

    return categories


# =============================================================================
# Intent-based Keyword Split (v8.4)
# =============================================================================


def is_commercial_keyword(keyword: str, lang: str = "ru") -> bool:
    """Check if keyword has commercial intent (–∫—É–ø–∏—Ç—å, —Ü–µ–Ω–∞, etc.)."""
    kw_lower = keyword.lower()
    modifiers = get_commercial_modifiers(lang)
    return any(mod in kw_lower for mod in modifiers)


def split_keywords_by_intent(keywords: list[dict], lang: str = "ru") -> tuple[list[dict], list[dict]]:
    """
    Split keywords into core (topic/editorial) and commercial (transactional).

    Core keywords: topic keywords without commercial modifiers
    Commercial keywords: contain –∫—É–ø–∏—Ç—å/—Ü–µ–Ω–∞/–∑–∞–∫–∞–∑–∞—Ç—å/etc.

    Returns:
        Tuple[core_keywords, commercial_keywords]
    """
    core = []
    commercial = []

    for kw in keywords:
        if is_commercial_keyword(kw["keyword"], lang):
            commercial.append(kw)
        else:
            core.append(kw)

    return core, commercial


# =============================================================================
# Analysis Functions (Google 2025 Approach)
# =============================================================================


def analyze_keywords(keywords: list[dict], lang: str = "ru") -> dict:
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–µ–º–∞–Ω—Ç–∏–∫—É –∫–∞—Ç–µ–≥–æ—Ä–∏–∏.

    Args:
        keywords: List of keyword dicts
        lang: Language code ("ru" or "uk")

    Returns:
        Dict —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏
    """
    if not keywords:
        return {
            "count": 0,
            "total_volume": 0,
            "semantic_depth": "empty",
            "content_format": "none",
            "recommended_words": "0",
            "need_clustering": False,
            "all_keywords": [],
            "core_keywords": [],
            "commercial_keywords": [],
        }

    count = len(keywords)
    total_volume = sum(kw["volume"] for kw in keywords)

    # Sort by volume
    sorted_kws = sorted(keywords, key=lambda x: -x["volume"])

    # High volume keywords (>100)
    high_volume = [kw for kw in keywords if kw["volume"] > 100]

    # Split by intent (v8.4)
    core_keywords, commercial_keywords = split_keywords_by_intent(keywords, lang)

    # Primary keyword (topic-first): prefer core intent over commercial modifiers.
    if core_keywords:
        primary = sorted(core_keywords, key=lambda x: -x["volume"])[0]
    else:
        primary = sorted_kws[0]

    # Determine semantic depth
    if count <= 5:
        semantic_depth = "shallow"
        content_format = "compact"
        recommended_words = "150-250"
    elif count <= 15:
        semantic_depth = "medium"
        content_format = "standard"
        recommended_words = "250-400"
    else:
        semantic_depth = "deep"
        content_format = "comprehensive"
        recommended_words = "400-600"

    # Need clustering?
    need_clustering = count > 20

    return {
        "count": count,
        "total_volume": total_volume,
        "primary": primary,
        "high_volume_count": len(high_volume),
        "commercial_count": len(commercial_keywords),
        "core_count": len(core_keywords),
        "semantic_depth": semantic_depth,
        "content_format": content_format,
        "recommended_words": recommended_words,
        "need_clustering": need_clustering,
        "all_keywords": sorted_kws,
        # v8.4: Split by intent
        "core_keywords": core_keywords,
        "commercial_keywords": commercial_keywords,
    }


def generate_content_guidelines(analysis: dict) -> dict:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞.

    Google 2025 –ø–æ–¥—Ö–æ–¥:
    - Editorial content (buying guide)
    - Depth over length
    - Intent matching
    """
    depth = analysis["semantic_depth"]
    count = analysis["count"]

    # Base structure (–≤—Å–µ–≥–¥–∞)
    structure = {
        "h1": True,
        "intro": {
            "required": True,
            "purpose": "–ß—Ç–æ —ç—Ç–æ + –∑–∞—á–µ–º –Ω—É–∂–Ω–æ",
            "length": "2-4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è",
        },
        "buying_advice": {
            "required": True,
            "purpose": "–ö–∞–∫ –≤—ã–±—Ä–∞—Ç—å / –Ω–∞ —á—Ç–æ —Å–º–æ—Ç—Ä–µ—Ç—å",
            "format": "–±—É–ª–ª–µ—Ç—ã",
        },
        "trust_signals": {"required": True, "items": ["–¥–æ—Å—Ç–∞–≤–∫–∞", "–Ω–∞–ª–∏—á–∏–µ", "–æ–ø–ª–∞—Ç–∞"]},
    }

    # Adaptive elements based on semantic depth
    if depth == "shallow":
        structure["faq"] = {
            "required": False,
            "count": "0-1",
            "note": "–¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å",
        }
        structure["comparison_table"] = {"required": False}
        structure["types_section"] = {"required": False}

    elif depth == "medium":
        structure["faq"] = {"required": True, "count": "2-3", "note": "–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã"}
        structure["comparison_table"] = {"required": False}
        structure["types_section"] = {
            "required": count > 10,
            "note": "–ï—Å–ª–∏ –µ—Å—Ç—å –ø–æ–¥—Ç–∏–ø—ã —Ç–æ–≤–∞—Ä–æ–≤",
        }

    else:  # deep
        structure["faq"] = {"required": True, "count": "3-5", "note": "–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã"}
        structure["comparison_table"] = {"required": True, "note": "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–∏–ø–æ–≤/—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫"}
        structure["types_section"] = {"required": True, "note": "–í–∏–¥—ã —Ç–æ–≤–∞—Ä–æ–≤ —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º"}

    # Keyword requirements
    coverage_target = get_guidelines_coverage_target(count)
    keyword_requirements = {
        "primary_in_h1": True,
        "primary_in_intro": True,
        "coverage_target": f"{coverage_target}%",
        "density": "–Ω–µ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º (natural writing)",
        "note": "–ï—Å–ª–∏ —Ç–µ–º–∞ —Ä–∞—Å–∫—Ä—ã—Ç–∞ ‚Äî –∫–ª—é—á–∏ –ø–æ—è–≤—è—Ç—Å—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ",
    }

    # Quality requirements (Google 2025)
    quality = {
        "water_percent": {"min": 40, "max": 65},
        "nausea_classic": {"max": 3.5},
        "blacklist_phrases": 0,
        "unique_content": True,
        "editorial_tone": "—ç–∫—Å–ø–µ—Ä—Ç –∞–≤—Ç–æ-–¥–µ—Ç–µ–π–ª–∏–Ω–≥–∞",
    }

    return {
        "structure": structure,
        "keyword_requirements": keyword_requirements,
        "quality": quality,
        "length_note": "LLM –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –¥–ª–∏–Ω—É –ø–æ —Ç–µ–º–µ. –ù–µ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ–±—ä—ë–º.",
    }


def analyze_category(slug: str, lang: str = "ru") -> dict:
    """
    –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è LLM.
    Uses D+E Fallback Pattern to load keywords.

    Args:
        slug: Category slug
        lang: Language code ("ru" or "uk")
    """
    # Get L3 name for display (use correct mapping for lang)
    try:
        from scripts.seo_utils import get_mappings_for_lang
    except ImportError:
        from seo_utils import get_mappings_for_lang
    _, slug_to_l3 = get_mappings_for_lang(lang)
    l3_name = slug_to_l3.get(slug, slug)

    # Load keywords using D+E Fallback
    keywords_raw, source, extra_data = load_keywords_for_slug(slug, lang)

    if not keywords_raw:
        return {"error": f"No keywords found for slug: {slug} (lang={lang}). Run seo-clean or parse CSV first."}

    # Analyze
    keyword_analysis = analyze_keywords(keywords_raw, lang)
    content_guidelines = generate_content_guidelines(keyword_analysis)

    # Determine if clustering is needed (only for raw sources)
    needs_clean = source in ["raw_json", "csv"] and keyword_analysis["count"] > 15

    # Build result
    result = {
        "meta": {
            "slug": slug,
            "category_name": l3_name,
            "analyzed_at": datetime.now(UTC).isoformat(),
            "approach": "Google 2025 (Depth over Length)",
            "source": source,
            "needs_clean": needs_clean,
        },
        "keywords": {
            "count": keyword_analysis["count"],
            "total_volume": keyword_analysis["total_volume"],
            "primary": keyword_analysis["primary"],
            "high_volume_count": keyword_analysis["high_volume_count"],
            "commercial_count": keyword_analysis["commercial_count"],
            "core_count": keyword_analysis["core_count"],
            "semantic_depth": keyword_analysis["semantic_depth"],
            "need_clustering": keyword_analysis["need_clustering"],
            "all_keywords": keyword_analysis["all_keywords"][:20],  # Top 20 for display
            # v8.4: Split by intent for coverage calculation
            "core_keywords": [kw["keyword"] for kw in keyword_analysis["core_keywords"]],
            "commercial_keywords": [kw["keyword"] for kw in keyword_analysis["commercial_keywords"]],
        },
        "content": {
            "format": keyword_analysis["content_format"],
            "recommended_words": keyword_analysis["recommended_words"],
            "structure": content_guidelines["structure"],
            "keyword_requirements": content_guidelines["keyword_requirements"],
            "quality": content_guidelines["quality"],
            "length_note": content_guidelines["length_note"],
        },
        "llm_prompt_hints": {
            "page_type": "–∫–∞—Ç–µ–≥–æ—Ä–∏—è –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞–≥–∞–∑–∏–Ω–∞",
            "content_type": "buying guide (–Ω–µ SEO-—Ç–µ–∫—Å—Ç)",
            "tone": "—ç–∫—Å–ø–µ—Ä—Ç –∞–≤—Ç–æ-–¥–µ—Ç–µ–π–ª–∏–Ω–≥–∞",
            "goal": "–ø–æ–º–æ—á—å –≤—ã–±—Ä–∞—Ç—å —Ç–æ–≤–∞—Ä",
            "avoid": ["–≤–æ–¥–∞ —Ä–∞–¥–∏ –æ–±—ä—ë–º–∞", "—É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ —Ñ—Ä–∞–∑—ã", "–∏—Å—Ç–æ—Ä–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏", "AI-fluff"],
        },
    }

    # Add extra data from clean.json if available
    if extra_data:
        result["seo_titles"] = extra_data.get("seo_titles")
        result["entity_dictionary"] = extra_data.get("entity_dictionary")
        result["content_rules"] = extra_data.get("content_rules")
        result["clustering_stats"] = extra_data.get("stats")

    return result


# =============================================================================
# CLI
# =============================================================================


def list_all_categories():
    """–ü–æ–∫–∞–∑–∞—Ç—å –≤—Å–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å –∞–Ω–∞–ª–∏–∑–æ–º."""
    print("=" * 70)
    print("–ê–ù–ê–õ–ò–ó –í–°–ï–• –ö–ê–¢–ï–ì–û–†–ò–ô (Google 2025 Approach)")
    print("=" * 70)
    print()

    categories = read_semantics_csv(str(SEMANTICS_CSV))

    # Analyze all
    results = []
    for l3_name, keywords in categories.items():
        slug = L3_TO_SLUG.get(l3_name, "???")
        analysis = analyze_keywords(keywords)
        results.append(
            {
                "slug": slug,
                "name": l3_name,
                "count": analysis["count"],
                "volume": analysis["total_volume"],
                "depth": analysis["semantic_depth"],
                "format": analysis["content_format"],
                "words": analysis["recommended_words"],
                "clustering": analysis["need_clustering"],
            }
        )

    # Sort by keyword count
    results.sort(key=lambda x: -x["count"])

    # Print table
    print(f"{'Slug':<25} {'Keywords':>8} {'Volume':>7} {'Depth':<10} {'Format':<12} {'Words':<10} {'Cluster'}")
    print("-" * 90)

    for r in results:
        cluster = "YES" if r["clustering"] else "no"
        print(
            f"{r['slug']:<25} {r['count']:>8} {r['volume']:>7} {r['depth']:<10} {r['format']:<12} {r['words']:<10} {cluster}"
        )

    print("-" * 90)
    print(f"Total: {len(results)} categories")
    print()
    print("Semantic Depth:")
    print("  shallow (‚â§5 kw)  ‚Üí compact format, 150-250 words")
    print("  medium (6-15 kw) ‚Üí standard format, 250-400 words")
    print("  deep (16+ kw)    ‚Üí comprehensive format, 400-600 words")


def main():
    if len(sys.argv) < 2:
        print("Usage:")
        print("  python3 scripts/analyze_category.py <slug>")
        print("  python3 scripts/analyze_category.py <slug> --json")
        print("  python3 scripts/analyze_category.py <slug> --lang uk")
        print("  python3 scripts/analyze_category.py --list")
        print()
        print("Example:")
        print("  python3 scripts/analyze_category.py antibitum")
        print("  python3 scripts/analyze_category.py aktivnaya-pena --json")
        print("  python3 scripts/analyze_category.py aktivnaya-pena --lang uk --json")
        sys.exit(1)

    if sys.argv[1] == "--list":
        list_all_categories()
        sys.exit(0)

    slug = sys.argv[1]
    output_json = "--json" in sys.argv

    # Parse --lang parameter
    lang = "ru"
    if "--lang" in sys.argv:
        lang_idx = sys.argv.index("--lang")
        if lang_idx + 1 < len(sys.argv):
            lang = sys.argv[lang_idx + 1]

    result = analyze_category(slug, lang)

    if "error" in result:
        print(f"‚ùå {result['error']}")
        sys.exit(1)

    if output_json:
        print(json.dumps(result, ensure_ascii=False, indent=2))
    else:
        # Human-readable output
        print()
        print("=" * 60)
        print(f"–ê–ù–ê–õ–ò–ó –ö–ê–¢–ï–ì–û–†–ò–ò: {result['meta']['category_name']}")
        print(f"Slug: {result['meta']['slug']}")
        print("=" * 60)
        print()

        # Source info
        meta = result["meta"]
        source_emoji = {"clean_json": "‚úì", "raw_json": "‚óã", "csv": "‚ñ≥"}
        source_label = {
            "clean_json": "Clean JSON (clustered)",
            "raw_json": "Raw JSON",
            "csv": "CSV fallback",
        }
        print(f"üìÇ SOURCE: {source_emoji.get(meta['source'], '?')} {source_label.get(meta['source'], meta['source'])}")
        if meta.get("needs_clean"):
            print(f"   ‚ö†Ô∏è  –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è: clean {result['meta']['slug']}")
        print()

        kw = result["keywords"]
        print("üìä KEYWORDS:")
        print(f"   Count: {kw['count']}")
        print(f"   Total Volume: {kw['total_volume']}")
        print(f'   Primary: "{kw["primary"]["keyword"]}" (vol: {kw["primary"]["volume"]})')
        print(f"   High Volume (>100): {kw['high_volume_count']}")
        print(f"   ‚îå‚îÄ Core (topic): {kw['core_count']} ‚Üê target coverage")
        print(f"   ‚îî‚îÄ Commercial: {kw['commercial_count']} (INFO only)")
        print(f"   Semantic Depth: {kw['semantic_depth']}")
        print(f"   Need Clustering: {'Yes' if kw['need_clustering'] else 'No'}")
        print()

        # Show clustering stats if from clean.json
        if result.get("clustering_stats"):
            stats = result["clustering_stats"]
            print("üîÑ CLUSTERING STATS:")
            print(f"   Before: {stats['before']} ‚Üí After: {stats['after']} keywords")
            if "reduction_percent" in stats:
                print(f"   Reduction: {stats['reduction_percent']}%")
            if "clusters_count" in stats:
                print(f"   Clusters: {stats['clusters_count']}")
            print()

        # Show SEO titles if available
        if result.get("seo_titles"):
            titles = result["seo_titles"]
            print("üìå SEO TITLES:")
            print(f'   H1: "{titles["h1"]}" (vol: {titles["h1_volume"]})')
            print(f'   Main keyword: "{titles["main_keyword"]}" (vol: {titles["main_keyword_volume"]})')
            print()

        content = result["content"]
        print("üìù CONTENT RECOMMENDATIONS:")
        print(f"   Format: {content['format']}")
        print(f"   Words: {content['recommended_words']}")
        print(f"   Coverage Target: {content['keyword_requirements']['coverage_target']}")
        print()

        struct = content["structure"]
        print("üìã STRUCTURE:")
        print("   H1: Required")
        print(f"   Intro: {struct['intro']['length']}")
        print("   Buying Advice: –±—É–ª–ª–µ—Ç—ã")
        if struct.get("faq", {}).get("required"):
            print(f"   FAQ: {struct['faq']['count']} –≤–æ–ø—Ä–æ—Å–æ–≤")
        if struct.get("comparison_table", {}).get("required"):
            print("   –¢–∞–±–ª–∏—Ü–∞: Required")
        if struct.get("types_section", {}).get("required"):
            print("   –í–∏–¥—ã —Ç–æ–≤–∞—Ä–æ–≤: Required")
        print()

        # Show entity dictionary if available
        if result.get("entity_dictionary"):
            print("üìñ ENTITY DICTIONARY:")
            for key, values in result["entity_dictionary"].items():
                print(f"   {key}: {', '.join(values[:3])}{'...' if len(values) > 3 else ''}")
            print()

        print(f"‚ö†Ô∏è  NOTE: {content['length_note']}")
        print()


if __name__ == "__main__":
    main()
