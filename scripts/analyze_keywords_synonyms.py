#!/usr/bin/env python3
"""–ê–Ω–∞–ª–∏–∑ keywords –∏ synonyms —Å–æ–≥–ª–∞—Å–Ω–æ CONTENT_GUIDE.md."""

import json
from datetime import datetime
from pathlib import Path

ROOT = Path(__file__).parent.parent
CATEGORIES_DIR = ROOT / "categories"
OUTPUT_FILE = ROOT / "tasks" / "active" / "KEYWORDS_SYNONYMS_TZ.md"

# –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∏–∑ CONTENT_GUIDE.md
REQUIREMENTS = {
    "keywords_min": 1,  # –ú–∏–Ω–∏–º—É–º 1 keyword (–≥–ª–∞–≤–Ω—ã–π)
    "synonyms_min": 3,  # –ú–∏–Ω–∏–º—É–º 3 —Å–∏–Ω–æ–Ω–∏–º–∞
    "synonyms_meta_only": 1,  # –ú–∏–Ω–∏–º—É–º 1 –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–π (meta_only)
    "entities_min": 5,  # –ú–∏–Ω–∏–º—É–º 5 entities
    "micro_intents_min": 3,  # –ú–∏–Ω–∏–º—É–º 3 micro_intents
}


def analyze_category(clean_file: Path) -> dict:
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç _clean.json –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º."""
    data = json.loads(clean_file.read_text(encoding="utf-8"))
    slug = data.get("id", clean_file.stem.replace("_clean", ""))
    name = data.get("name", slug)

    keywords = data.get("keywords", [])
    synonyms = data.get("synonyms", [])
    entities = data.get("entities", [])
    micro_intents = data.get("micro_intents", [])

    # –†–∞–∑–¥–µ–ª—è–µ–º synonyms
    regular_synonyms = [s for s in synonyms if s.get("use_in") != "meta_only"]
    meta_only_synonyms = [s for s in synonyms if s.get("use_in") == "meta_only"]

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º volume=0
    zero_keywords = [k for k in keywords if k.get("volume", 0) == 0]
    zero_synonyms = [s for s in regular_synonyms if s.get("volume", 0) == 0]

    issues = []
    warnings = []

    # Keywords checks
    if len(keywords) < REQUIREMENTS["keywords_min"]:
        issues.append(f"keywords: {len(keywords)} < {REQUIREMENTS['keywords_min']} (–Ω–µ—Ç –≥–ª–∞–≤–Ω–æ–≥–æ –∫–ª—é—á–∞)")
    if zero_keywords:
        warnings.append(f"keywords —Å volume=0: {len(zero_keywords)}")

    # Synonyms checks
    if len(regular_synonyms) < REQUIREMENTS["synonyms_min"]:
        issues.append(f"synonyms: {len(regular_synonyms)} < {REQUIREMENTS['synonyms_min']}")
    if len(meta_only_synonyms) < REQUIREMENTS["synonyms_meta_only"]:
        issues.append(
            f"meta_only synonyms: {len(meta_only_synonyms)} < {REQUIREMENTS['synonyms_meta_only']} (–Ω–µ—Ç –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏—Ö)"
        )
    if zero_synonyms:
        warnings.append(f"synonyms —Å volume=0: {len(zero_synonyms)}")

    # Entities checks
    if len(entities) < REQUIREMENTS["entities_min"]:
        issues.append(f"entities: {len(entities)} < {REQUIREMENTS['entities_min']}")

    # Micro-intents checks
    if len(micro_intents) < REQUIREMENTS["micro_intents_min"]:
        issues.append(f"micro_intents: {len(micro_intents)} < {REQUIREMENTS['micro_intents_min']}")

    return {
        "slug": slug,
        "name": name,
        "file": str(clean_file.relative_to(ROOT)),
        "keywords": keywords,
        "keywords_count": len(keywords),
        "synonyms_count": len(regular_synonyms),
        "meta_only_count": len(meta_only_synonyms),
        "entities_count": len(entities),
        "micro_intents_count": len(micro_intents),
        "entities": entities,
        "micro_intents": micro_intents,
        "issues": issues,
        "warnings": warnings,
        "score": calculate_score(
            len(keywords), len(regular_synonyms), len(meta_only_synonyms), len(entities), len(micro_intents)
        ),
    }


def calculate_score(kw, syn, meta, ent, mi) -> int:
    """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç score –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω–æ—Å—Ç–∏ (0-100%)."""
    score = 0
    score += min(kw / REQUIREMENTS["keywords_min"], 1) * 20
    score += min(syn / REQUIREMENTS["synonyms_min"], 1) * 20
    score += min(meta / REQUIREMENTS["synonyms_meta_only"], 1) * 20
    score += min(ent / REQUIREMENTS["entities_min"], 1) * 20
    score += min(mi / REQUIREMENTS["micro_intents_min"], 1) * 20
    return int(score)


def generate_tz(results: list) -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¢–ó –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown."""
    with_issues = [r for r in results if r["issues"]]
    ok_results = [r for r in results if not r["issues"]]

    lines = [
        "# –¢–ó: –ê—É–¥–∏—Ç keywords –∏ synonyms",
        "",
        f"**–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è:** {datetime.now().strftime('%Y-%m-%d %H:%M')}",
        "**–°—Ç–∞—Ç—É—Å:** ‚¨ú –í —Ä–∞–±–æ—Ç–µ",
        "",
        "---",
        "",
        "## –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è (–∏–∑ CONTENT_GUIDE.md)",
        "",
        "| –ü–æ–ª–µ | –ú–∏–Ω–∏–º—É–º | –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ |",
        "|------|---------|------------|",
        f"| `keywords` | {REQUIREMENTS['keywords_min']}+ | –¢–û–ü –í–ß –∫–ª—é—á–∏ ‚Üí Title, H1 |",
        f"| `synonyms` | {REQUIREMENTS['synonyms_min']}+ | –í–∞—Ä–∏–∞—Ü–∏–∏ ‚Üí H2, —Ç–µ–∫—Å—Ç |",
        f"| `meta_only` | {REQUIREMENTS['synonyms_meta_only']}+ | –ö–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ (–∫—É–ø–∏—Ç—å, —Ü–µ–Ω–∞) ‚Üí Title —Ö–≤–æ—Å—Ç, Description |",
        f"| `entities` | {REQUIREMENTS['entities_min']}+ | E-E-A-T —Ç–µ—Ä–º–∏–Ω—ã ‚Üí —Ç–µ–∫—Å—Ç |",
        f"| `micro_intents` | {REQUIREMENTS['micro_intents_min']}+ | –í–æ–ø—Ä–æ—Å—ã ‚Üí FAQ –±–ª–æ–∫–∏ |",
        "",
        "---",
        "",
        "## –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞",
        "",
        f"- –í—Å–µ–≥–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π: **{len(results)}**",
        f"- –¢—Ä–µ–±—É—é—Ç –¥–æ—Ä–∞–±–æ—Ç–∫–∏: **{len(with_issues)}**",
        f"- –ü–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–ø–æ–ª–Ω–µ–Ω—ã: **{len(ok_results)}**",
        "",
        "### –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ score",
        "",
    ]

    # Score distribution
    score_ranges = {"100%": 0, "80-99%": 0, "60-79%": 0, "40-59%": 0, "<40%": 0}
    for r in results:
        if r["score"] == 100:
            score_ranges["100%"] += 1
        elif r["score"] >= 80:
            score_ranges["80-99%"] += 1
        elif r["score"] >= 60:
            score_ranges["60-79%"] += 1
        elif r["score"] >= 40:
            score_ranges["40-59%"] += 1
        else:
            score_ranges["<40%"] += 1

    for range_name, count in score_ranges.items():
        bar = "‚ñà" * (count // 2) if count else ""
        lines.append(f"- {range_name}: **{count}** {bar}")

    lines.extend(
        [
            "",
            "---",
            "",
            "## –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è –¥–æ—Ä–∞–±–æ—Ç–∫–∏",
            "",
        ]
    )

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ score (—Ö—É–¥—à–∏–µ –ø–µ—Ä–≤—ã–µ)
    for r in sorted(with_issues, key=lambda x: x["score"]):
        status_icon = "üî¥" if r["score"] < 40 else "üü°" if r["score"] < 80 else "üü¢"
        lines.append(f"### {status_icon} {r['name']} (`{r['slug']}`) ‚Äî {r['score']}%")
        lines.append("")
        lines.append(f"**–§–∞–π–ª:** `{r['file']}`")
        lines.append("")

        # –¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        lines.append("**–¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ:**")
        lines.append("| –ü–æ–ª–µ | –ï—Å—Ç—å | –ù—É–∂–Ω–æ | –°—Ç–∞—Ç—É—Å |")
        lines.append("|------|------|-------|--------|")

        kw_status = "‚úÖ" if r["keywords_count"] >= REQUIREMENTS["keywords_min"] else "‚ùå"
        syn_status = "‚úÖ" if r["synonyms_count"] >= REQUIREMENTS["synonyms_min"] else "‚ùå"
        meta_status = "‚úÖ" if r["meta_only_count"] >= REQUIREMENTS["synonyms_meta_only"] else "‚ùå"
        ent_status = "‚úÖ" if r["entities_count"] >= REQUIREMENTS["entities_min"] else "‚ùå"
        mi_status = "‚úÖ" if r["micro_intents_count"] >= REQUIREMENTS["micro_intents_min"] else "‚ùå"

        lines.append(f"| keywords | {r['keywords_count']} | {REQUIREMENTS['keywords_min']}+ | {kw_status} |")
        lines.append(f"| synonyms | {r['synonyms_count']} | {REQUIREMENTS['synonyms_min']}+ | {syn_status} |")
        lines.append(f"| meta_only | {r['meta_only_count']} | {REQUIREMENTS['synonyms_meta_only']}+ | {meta_status} |")
        lines.append(f"| entities | {r['entities_count']} | {REQUIREMENTS['entities_min']}+ | {ent_status} |")
        lines.append(
            f"| micro_intents | {r['micro_intents_count']} | {REQUIREMENTS['micro_intents_min']}+ | {mi_status} |"
        )
        lines.append("")

        # –ì–ª–∞–≤–Ω—ã–π –∫–ª—é—á
        if r["keywords"]:
            main_kw = r["keywords"][0]
            lines.append(f"**–ì–ª–∞–≤–Ω—ã–π –∫–ª—é—á:** `{main_kw['keyword']}` (volume: {main_kw.get('volume', 0)})")
            lines.append("")

        # –ü—Ä–æ–±–ª–µ–º—ã
        if r["issues"]:
            lines.append("**–ü—Ä–æ–±–ª–µ–º—ã:**")
            for issue in r["issues"]:
                lines.append(f"- ‚ùå {issue}")
            lines.append("")

        if r["warnings"]:
            lines.append("**–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è:**")
            for warn in r["warnings"]:
                lines.append(f"- ‚ö†Ô∏è {warn}")
            lines.append("")

        # –ß–µ–∫–ª–∏—Å—Ç
        lines.append("**–ß–µ–∫–ª–∏—Å—Ç:**")
        if r["keywords_count"] < REQUIREMENTS["keywords_min"]:
            lines.append("- [ ] –î–æ–±–∞–≤–∏—Ç—å –≥–ª–∞–≤–Ω—ã–π keyword (—Å–∞–º—ã–π —á–∞—Å—Ç–æ—Ç–Ω—ã–π)")
        if r["synonyms_count"] < REQUIREMENTS["synonyms_min"]:
            need = REQUIREMENTS["synonyms_min"] - r["synonyms_count"]
            lines.append(f"- [ ] –î–æ–±–∞–≤–∏—Ç—å {need}+ synonyms (–≤–∞—Ä–∏–∞—Ü–∏–∏ –∫–ª—é—á–∞)")
        if r["meta_only_count"] < REQUIREMENTS["synonyms_meta_only"]:
            lines.append("- [ ] –î–æ–±–∞–≤–∏—Ç—å –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ synonyms —Å `use_in: meta_only` (–∫—É–ø–∏—Ç—å X, X —Ü–µ–Ω–∞)")
        if r["entities_count"] < REQUIREMENTS["entities_min"]:
            need = REQUIREMENTS["entities_min"] - r["entities_count"]
            lines.append(f"- [ ] –î–æ–±–∞–≤–∏—Ç—å {need}+ entities (—Ç–µ—Ä–º–∏–Ω—ã, –±—Ä–µ–Ω–¥—ã, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏)")
        if r["micro_intents_count"] < REQUIREMENTS["micro_intents_min"]:
            need = REQUIREMENTS["micro_intents_min"] - r["micro_intents_count"]
            lines.append(f"- [ ] –î–æ–±–∞–≤–∏—Ç—å {need}+ micro_intents (–≤–æ–ø—Ä–æ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π)")
        lines.append("- [ ] –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
        lines.append("")
        lines.append("---")
        lines.append("")

    # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ OK
    if ok_results:
        lines.append("## –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ 100% (–ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–ø–æ–ª–Ω–µ–Ω—ã)")
        lines.append("")
        for r in sorted(ok_results, key=lambda x: x["slug"]):
            main_kw = r["keywords"][0]["keyword"] if r["keywords"] else "‚Äî"
            lines.append(f"- ‚úÖ **{r['name']}** (`{r['slug']}`) ‚Äî `{main_kw}`")
        lines.append("")

    return "\n".join(lines)


def main():
    print("üîç –ê—É–¥–∏—Ç keywords –∏ synonyms...\n")

    results = []
    for clean_file in sorted(CATEGORIES_DIR.rglob("*_clean.json")):
        result = analyze_category(clean_file)
        results.append(result)
        if result["issues"]:
            print(
                f"{'üî¥' if result['score'] < 40 else 'üü°'} {result['slug']}: {result['score']}% ‚Äî {len(result['issues'])} –ø—Ä–æ–±–ª–µ–º"
            )

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¢–ó
    tz_content = generate_tz(results)
    OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)
    OUTPUT_FILE.write_text(tz_content, encoding="utf-8")

    with_issues = [r for r in results if r["issues"]]
    avg_score = sum(r["score"] for r in results) / len(results) if results else 0

    print(f"\n{'=' * 50}")
    print(f"‚úÖ –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {len(results)} –∫–∞—Ç–µ–≥–æ—Ä–∏–π")
    print(f"‚ö†Ô∏è  –¢—Ä–µ–±—É—é—Ç –¥–æ—Ä–∞–±–æ—Ç–∫–∏: {len(with_issues)}")
    print(f"üìä –°—Ä–µ–¥–Ω–∏–π score: {avg_score:.0f}%")
    print(f"\nüìÑ –¢–ó —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {OUTPUT_FILE}")


if __name__ == "__main__":
    main()
