"""
DEPRECATED: Use validate_content.py instead.

This script is kept for backwards compatibility only.
All functionality has been migrated to validate_content.py (SSOT).

Migration:
    OLD: python3 scripts/check_simple_v2_md.py file.md "keyword" B
    NEW: python3 scripts/validate_content.py file.md "keyword"

---

SEO Validator v2.0 MD - Google 2025 Compatible
–í–∞–ª–∏–¥–∞—Ç–æ—Ä Markdown –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å YAML front matter

UPDATED: –ò—Å–ø–æ–ª—å–∑—É–µ—Ç seo_utils.py –¥–ª—è —É–Ω–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å RULES 2025
"""

import json
import re
import sys
from pathlib import Path
from typing import Dict, List, Tuple

import yaml

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∏–∑ seo_utils.py –¥–ª—è —É–Ω–∏—Ñ–∏–∫–∞—Ü–∏–∏
try:
    from seo_utils import (
        count_chars_no_spaces,
        normalize_text,
        count_words,
        get_tier_requirements
    )
    UTILS_AVAILABLE = True
except ImportError:
    # Fallback –µ—Å–ª–∏ seo_utils.py –Ω–µ –≤ –ø—É—Ç–∏
    UTILS_AVAILABLE = False
    print("‚ö†Ô∏è  WARNING: seo_utils.py –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ª–æ–∫–∞–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è")

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º Nausea calculator (v7.1)
try:
    from check_water_natasha import calculate_metrics_from_text
    NAUSEA_AVAILABLE = True
except ImportError:
    NAUSEA_AVAILABLE = False


def parse_markdown_file(md_file: str) -> Tuple[Dict, str]:
    """
    –ü–∞—Ä—Å–∏–Ω–≥ MD —Ñ–∞–π–ª–∞ —Å YAML front matter

    Returns:
        (metadata_dict, markdown_content)
    """
    with open(md_file, encoding="utf-8") as f:
        content = f.read()

    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫ (Windows CRLF ‚Üí Unix LF)
    content = content.replace("\r\n", "\n").replace("\r", "\n")

    # –ò–∑–≤–ª–µ–∫–∞–µ–º YAML front matter
    yaml_match = re.match(r"^---\s*\n(.*?)\n---\s*\n(.*)$", content, re.DOTALL)

    if yaml_match:
        yaml_str = yaml_match.group(1)
        md_content = yaml_match.group(2)
        try:
            metadata = yaml.safe_load(yaml_str)
        except yaml.YAMLError:
            metadata = {}
    else:
        metadata = {}
        md_content = content

    return metadata, md_content


def extract_text_content(md: str) -> str:
    """
    –ò–∑–≤–ª–µ—á—å —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç –∏–∑ Markdown

    UNIFIED: –ò—Å–ø–æ–ª—å–∑—É–µ—Ç normalize_text() –∏–∑ seo_utils.py –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
    """
    if UTILS_AVAILABLE:
        return normalize_text(md)

    # Fallback - –ª–æ–∫–∞–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è
    # –£–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ [text](url) -> text
    text = re.sub(r"\[([^\]]+)\]\([^\)]+\)", r"\1", md)
    # –£–±–∏—Ä–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ #
    text = re.sub(r"^#{1,6}\s+", "", text, flags=re.MULTILINE)
    # –£–±–∏—Ä–∞–µ–º —Ç–∞–±–ª–∏—Ü—ã
    text = re.sub(r"\|[^\n]+\|", "", text)
    # –£–±–∏—Ä–∞–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ —Ç–∞–±–ª–∏—Ü
    text = re.sub(r"\|-+\|", "", text)
    # –£–±–∏—Ä–∞–µ–º –∂–∏—Ä–Ω—ã–π/–∫—É—Ä—Å–∏–≤
    text = re.sub(r"[*_]{1,2}([^*_]+)[*_]{1,2}", r"\1", text)
    # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r"\s+", " ", text).strip()
    return text


# NOTE: count_chars_no_spaces –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω –∏–∑ seo_utils.py
# –ï—Å–ª–∏ –∏–º–ø–æ—Ä—Ç –Ω–µ —É–¥–∞–ª—Å—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback
if not UTILS_AVAILABLE:
    def count_chars_no_spaces(content: str) -> int:
        """
        Fallback: –ü–æ–¥—Å—á—ë—Ç —Å–∏–º–≤–æ–ª–æ–≤ –ë–ï–ó –ø—Ä–æ–±–µ–ª–æ–≤, –ø–µ—Ä–µ–Ω–æ—Å–æ–≤ —Å—Ç—Ä–æ–∫ –∏ —Ç–∞–±–æ–≤

        EXACT FORMULA - —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å seo_utils.py
        """
        no_spaces = content.replace(' ', '').replace('\n', '').replace('\t', '').replace('\r', '')
        return len(no_spaces)


def find_matches_longest_first(text: str, keywords_data: Dict) -> Dict:
    """
    –ê–ª–≥–æ—Ä–∏—Ç–º Longest Match First –¥–ª—è –ø–æ–¥—Å—á—ë—Ç–∞ keyword density.

    –†–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É "—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∫–∞–Ω–Ω–∏–±–∞–ª–∏–∑–º–∞":
    - –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –¥–ª–∏–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã ("–∞–∫—Ç–∏–≤–Ω–∞—è –ø–µ–Ω–∞ –¥–ª—è –º–æ–π–∫–∏ –∞–≤—Ç–æ")
    - –ú–∞—Ä–∫–∏—Ä—É–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏
    - –ö–æ—Ä–æ—Ç–∫–∏–µ —Ñ—Ä–∞–∑—ã ("–ø–µ–Ω–∞") –ù–ï —Å—á–∏—Ç–∞—é—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ —É–∂–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö

    Returns:
        Dict —Å unique_matches –∏ per-keyword stats
    """
    text_lower = text.lower()

    # 1. –°–æ–±–∏—Ä–∞–µ–º –í–°–ï —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ñ—Ä–∞–∑—ã –∏–∑ –≤—Å–µ—Ö keywords
    all_phrases = set()
    phrase_to_keywords = {}  # phrase -> list of keyword objects that contain it

    for kw_type in ["primary", "secondary", "supporting"]:
        for kw_obj in keywords_data.get(kw_type, []):
            keyword = kw_obj.get("keyword", "").lower()
            variations = kw_obj.get("variations", {})
            exact_forms = variations.get("exact", [])

            # –î–æ–±–∞–≤–ª—è–µ–º —Å–∞–º keyword
            if keyword:
                all_phrases.add(keyword)
                if keyword not in phrase_to_keywords:
                    phrase_to_keywords[keyword] = []
                phrase_to_keywords[keyword].append({
                    "keyword": kw_obj.get("keyword"),
                    "type": kw_type,
                    "is_exact": True
                })

            # –î–æ–±–∞–≤–ª—è–µ–º exact variations
            for form in exact_forms:
                form_lower = form.lower()
                all_phrases.add(form_lower)
                if form_lower not in phrase_to_keywords:
                    phrase_to_keywords[form_lower] = []
                phrase_to_keywords[form_lower].append({
                    "keyword": kw_obj.get("keyword"),
                    "type": kw_type,
                    "is_exact": True
                })

    # 2. –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–ª–∏–Ω–µ (longest first)
    sorted_phrases = sorted(all_phrases, key=len, reverse=True)

    # 3. –ò—â–µ–º matches, –∏—Å–∫–ª—é—á–∞—è –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
    used_ranges = []  # [(start, end), ...]
    unique_matches = []  # [{"phrase": ..., "start": ..., "end": ..., "keywords": [...]}]

    def is_overlapping(start: int, end: int) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –ø–µ—Ä–µ—Å–µ–∫–∞–µ—Ç—Å—è –ª–∏ –¥–∏–∞–ø–∞–∑–æ–Ω —Å —É–∂–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏"""
        for used_start, used_end in used_ranges:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ
            if not (end <= used_start or start >= used_end):
                return True
        return False

    for phrase in sorted_phrases:
        if len(phrase) < 3:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ
            continue

        pattern = r'\b' + re.escape(phrase) + r'\b'

        for match in re.finditer(pattern, text_lower):
            start, end = match.start(), match.end()

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –ø–µ—Ä–µ—Å–µ–∫–∞–µ—Ç—Å—è –ª–∏ —Å —É–∂–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏
            if not is_overlapping(start, end):
                used_ranges.append((start, end))
                unique_matches.append({
                    "phrase": phrase,
                    "start": start,
                    "end": end,
                    "keywords": phrase_to_keywords.get(phrase, [])
                })

    return {
        "unique_matches": unique_matches,
        "total_unique": len(unique_matches),
        "used_ranges": used_ranges
    }


def check_keyword_density_and_distribution(
    md_content: str, data_json_path: str, word_count: int, requirements: Dict = None
) -> Dict:
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ –∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è keywords ‚Äî v7.4 Longest Match First

    –ò–ó–ú–ï–ù–ï–ù–ò–Ø v7.4:
    - Longest Match First: –¥–ª–∏–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã –∏—â—É—Ç—Å—è –ø–µ—Ä–≤—ã–º–∏
    - –ö–æ—Ä–æ—Ç–∫–∏–µ —Ñ—Ä–∞–∑—ã –ù–ï —Å—á–∏—Ç–∞—é—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ —É–∂–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–ª–∏–Ω–Ω—ã—Ö
    - Total density —Å—á–∏—Ç–∞–µ—Ç—Å—è –ø–æ –£–ù–ò–ö–ê–õ–¨–ù–´–ú matches (–±–µ–∑ –∫–∞–Ω–Ω–∏–±–∞–ª–∏–∑–º–∞)
    - Per-keyword density ‚Äî –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è (–¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ coverage)

    –§–æ—Ä–º—É–ª–∞ density: (unique_matches / word_count) √ó 100

    Targets:
    - TOTAL density: ‚â§2% (ideal), ‚â§3.5% (max), >3.5% = SPAM
    - Coverage: 50-60% keywords found (–Ω–µ 90%+ –∏–∑-–∑–∞ overlapping!)

    Args:
        md_content: –ø–æ–ª–Ω—ã–π markdown –∫–æ–Ω—Ç–µ–Ω—Ç
        data_json_path: –ø—É—Ç—å –∫ JSON —Ñ–∞–π–ª—É —Å keywords
        word_count: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ
        requirements: —Å–ª–æ–≤–∞—Ä—å —Å —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º–∏ (–∏–∑ seo_utils)

    Returns:
        Dict with metrics: density, coverage, warnings, errors
    """
    result = {
        "total_density": 0.0,
        "coverage": 0.0,
        "keywords_found": 0,
        "keywords_total": 0,
        "warnings": [],
        "errors": [],
        "details": []
    }

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è JSON
    if not Path(data_json_path).exists():
        result["errors"].append(f"‚ùå JSON —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {data_json_path}")
        return result

    # –ß—Ç–µ–Ω–∏–µ keywords –∏–∑ JSON
    try:
        with open(data_json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except Exception as e:
        result["errors"].append(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è JSON: {e}")
        return result

    keywords_dict = data.get("keywords", {})
    if not keywords_dict:
        result["errors"].append("‚ùå –ù–µ—Ç keywords –≤ JSON")
        return result

    md_lower = md_content.lower()

    # === LONGEST MATCH FIRST ===
    lmf_result = find_matches_longest_first(md_content, keywords_dict)
    unique_matches = lmf_result["unique_matches"]

    # –°–æ–±–∏—Ä–∞–µ–º –∫–∞–∫–∏–µ keywords –±—ã–ª–∏ –Ω–∞–π–¥–µ–Ω—ã (–¥–ª—è coverage)
    keywords_found_set = set()
    for match in unique_matches:
        for kw_info in match["keywords"]:
            keywords_found_set.add(kw_info["keyword"])

    # === Per-keyword —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ (–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è) ===
    for kw_type in ["primary", "secondary", "supporting"]:
        keywords_list = keywords_dict.get(kw_type, [])

        for kw_obj in keywords_list:
            keyword = kw_obj.get("keyword", "")
            density_target_str = kw_obj.get("density_target", "0%")

            # –°—á–∏—Ç–∞–µ–º —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ —ç—Ç–æ—Ç keyword –Ω–∞–π–¥–µ–Ω –≤ unique matches
            kw_count = sum(
                1 for m in unique_matches
                if any(k["keyword"] == keyword for k in m["keywords"])
            )

            # Density –¥–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (–Ω–µ –¥–ª—è –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏)
            if word_count > 0:
                actual_density = (kw_count / word_count) * 100
            else:
                actual_density = 0.0

            # Coverage: keyword –Ω–∞–π–¥–µ–Ω —Ö–æ—Ç—è –±—ã —Ä–∞–∑?
            is_found = keyword in keywords_found_set
            if is_found:
                result["keywords_found"] += 1

            # –°—Ç–∞—Ç—É—Å –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
            kw_status = "‚úÖ" if is_found else "‚ö†Ô∏è"

            result["details"].append({
                "keyword": keyword,
                "type": kw_type,
                "exact": kw_count,  # –í –Ω–æ–≤–æ–π –ª–æ–≥–∏–∫–µ –≤—Å—ë —Å—á–∏—Ç–∞–µ—Ç—Å—è –∫–∞–∫ "exact"
                "partial": 0,
                "total": kw_count,
                "target": kw_obj.get("occurrences_target", 0),
                "density_actual": f"{actual_density:.2f}%",
                "density_target": density_target_str,
                "status": kw_status
            })

    result["keywords_total"] = (
        len(keywords_dict.get("primary", [])) +
        len(keywords_dict.get("secondary", [])) +
        len(keywords_dict.get("supporting", []))
    )

    # === Coverage (—Å —É—á—ë—Ç–æ–º overlapping keywords) ===
    # –ü—Ä–∏ 52 keywords —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º, —Ä–µ–∞–ª—å–Ω—ã–π target 50-60%, –Ω–µ 90%+
    if result["keywords_total"] > 0:
        result["coverage"] = (result["keywords_found"] / result["keywords_total"]) * 100

    # === Total Density (–ì–õ–ê–í–ù–ê–Ø –ú–ï–¢–†–ò–ö–ê) ===
    # –°—á–∏—Ç–∞–µ–º —Ç–æ–ª—å–∫–æ –£–ù–ò–ö–ê–õ–¨–ù–´–ï matches (–±–µ–∑ –∫–∞–Ω–Ω–∏–±–∞–ª–∏–∑–º–∞)
    if word_count > 0:
        result["total_density"] = (len(unique_matches) / word_count) * 100

    density = result["total_density"]
    coverage = result["coverage"]

    # === Thresholds v7.5 (–∞–¥–∞–ø—Ç–∏–≤–Ω–æ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É keywords) ===
    # Clean JSON (‚â§20 kw): density blocker = 5.0%
    # Raw JSON (>20 kw): density blocker = 3.5%
    keywords_total = result["keywords_total"]
    if keywords_total <= 20:
        # Clean JSON —Å 12-15 –∫–ª—é—á–∞–º–∏ ‚Äî –≤—ã—à–µ –ø–æ—Ä–æ–≥ density
        density_blocker = 5.0
        density_warning = 3.5
    else:
        # Raw JSON —Å 50+ –∫–ª—é—á–∞–º–∏ ‚Äî —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–æ—Ä–æ–≥
        density_blocker = 3.5
        density_warning = 2.5

    if density > density_blocker:
        result["errors"].append(
            f"‚ùå BLOCKER: Total density {density:.2f}% (>{density_blocker}% ‚Äî —Å–ø–∞–º)"
        )
    elif density > density_warning:
        result["warnings"].append(
            f"‚ö†Ô∏è REVIEW: Total density {density:.2f}% (–≤—ã—Å–æ–∫–æ–≤–∞—Ç–æ, target ‚â§{density_warning}%)"
        )

    # Coverage ‚Äî —Å–º—è–≥—á—ë–Ω–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –¥–ª—è overlapping keywords
    coverage_min = 40.0  # –ë—ã–ª–æ 50%, —Å–Ω–∏–∂–µ–Ω–æ –¥–ª—è overlapping
    coverage_max = 80.0  # >80% = –≤–æ–∑–º–æ–∂–Ω–æ –ø–µ—Ä–µ—Å–ø–∞–º

    if requirements:
        coverage_min = requirements.get("coverage", 0.40) * 100

    if coverage < coverage_min:
        result["warnings"].append(
            f"‚ö†Ô∏è Coverage {coverage:.1f}% (target ‚â•{coverage_min:.0f}%)"
        )
    elif coverage > coverage_max:
        result["warnings"].append(
            f"‚ö†Ô∏è Coverage {coverage:.1f}% (>80% ‚Äî –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞ –ø–µ—Ä–µ—Å–ø–∞–º)"
        )

    return result


def check_intro_structure(md: str, words: List[str]) -> Tuple[bool, str]:
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏–Ω—Ç—Ä–æ (–ø–µ—Ä–≤—ã–µ 100-150 —Å–ª–æ–≤)
    """
    intro_words = words[:150] if len(words) >= 150 else words
    intro_text = " ".join(intro_words)
    word_count = len(intro_words)

    if word_count < 100:
        return False, f"–ò–Ω—Ç—Ä–æ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ: {word_count} —Å–ª–æ–≤ (–Ω—É–∂–Ω–æ 100-150)"

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ AI-—à–∞–±–ª–æ–Ω—ã
    ai_patterns = [
        r"–≤ —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –º—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º",
        r"–¥–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å",
        r"–≤ –¥–∞–Ω–Ω–æ–º –º–∞—Ç–µ—Ä–∏–∞–ª–µ",
        r"–¥–∞–≤–∞–π—Ç–µ —Ä–∞–∑–±–µ—Ä—ë–º—Å—è",
        r"–≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º –º–∏—Ä–µ",
    ]

    ai_detected = []
    for pattern in ai_patterns:
        if re.search(pattern, intro_text.lower()):
            ai_detected.append(pattern)

    if ai_detected:
        return (
            False,
            f"‚ö†Ô∏è  AI-—à–∞–±–ª–æ–Ω—ã –≤ –∏–Ω—Ç—Ä–æ: {', '.join(ai_detected)}",
        )

    return True, f"‚úÖ –ò–Ω—Ç—Ä–æ: {word_count} —Å–ª–æ–≤, –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —è–∑—ã–∫"


def check_h2_intent_structure(md: str) -> Tuple[bool, str]:
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ H2 –≤ Markdown (## –∑–∞–≥–æ–ª–æ–≤–æ–∫)
    """
    h2_list = re.findall(r"^##\s+(.+)$", md, re.MULTILINE)

    if not h2_list:
        return False, "‚ùå –ù–µ—Ç H2 –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤"

    h2_count = len(h2_list)

    # Intent-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
    intent_patterns = [
        r"–∫–∞–∫ –≤—ã–±—Ä–∞—Ç—å",
        r"–∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å",
        r"–∫–∞–∫ –ø—Ä–∏–º–µ–Ω—è—Ç—å",
        r"—á—Ç–æ —Ç–∞–∫–æ–µ",
        r"–≤–∏–¥—ã",
        r"—Ç–∏–ø—ã",
        r"—á–µ–º –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è",
        r"–ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞",
        r"–∫—Ä–∏—Ç–µ—Ä–∏–∏ –≤—ã–±–æ—Ä–∞",
        r"—Å–æ–≤–µ—Ç—ã",
        r"—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏",
        r"–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è",
        r"–æ—à–∏–±–∫–∏",
        r"—á–∞—Å—Ç–æ –∑–∞–¥–∞–≤–∞–µ–º—ã–µ",
    ]

    intent_h2_count = 0
    for h2 in h2_list:
        h2_lower = h2.lower()
        for pattern in intent_patterns:
            if re.search(pattern, h2_lower):
                intent_h2_count += 1
                break

    if intent_h2_count == 0:
        return (
            False,
            f"‚ö†Ô∏è  H2: {h2_count} —à—Ç, –Ω–æ –Ω–µ—Ç intent-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö",
        )

    return True, f"‚úÖ H2: {h2_count} —à—Ç, {intent_h2_count} –ø–æ–¥ –Ω–∞–º–µ—Ä–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"


def check_faq(md: str) -> Tuple[bool, str]:
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ FAQ (–≤–æ–ø—Ä–æ—Å—ã –≤ ### —Å ? –≤ –∫–æ–Ω—Ü–µ)
    """
    faq_questions = re.findall(r"^###\s+([^#\n]*\?[^#\n]*)$", md, re.MULTILINE)

    total_questions = len(faq_questions)

    if total_questions < 3:
        return False, f"‚ùå FAQ: {total_questions} –≤–æ–ø—Ä–æ—Å–æ–≤ (–Ω—É–∂–Ω–æ 3-6)"

    if total_questions > 6:
        return (
            False,
            f"‚ö†Ô∏è  FAQ: {total_questions} –≤–æ–ø—Ä–æ—Å–æ–≤ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 3-6)",
        )

    return True, f"‚úÖ FAQ: {total_questions} –≤–æ–ø—Ä–æ—Å–æ–≤"


def check_keyword_stuffing(text: str, keyword: str) -> Tuple[bool, str]:
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ keyword stuffing
    """
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫ (Windows CRLF ‚Üí Unix LF)
    text = text.replace("\r\n", "\n").replace("\r", "\n")

    text_lower = text.lower()
    keyword_lower = keyword.lower()

    matches = re.findall(r"\b" + re.escape(keyword_lower) + r"\b", text_lower)
    count = len(matches)

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –∞–±–∑–∞—Ü–∞–º (—Ä–∞–∑–¥–µ–ª–µ–Ω—ã –¥–≤—É–º—è \n)
    paragraphs = text.split("\n\n")
    for para in paragraphs:
        para_lower = para.lower()
        para_matches = re.findall(r"\b" + re.escape(keyword_lower) + r"\b", para_lower)
        if len(para_matches) >= 3:
            return (
                False,
                f"‚ùå Keyword stuffing: '{keyword}' –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è {len(para_matches)} —Ä–∞–∑ –≤ –æ–¥–Ω–æ–º –∞–±–∑–∞—Ü–µ",
            )

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—â–µ–π –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏
    words = text.split()
    if len(words) < 500 and count > 10:
        return (
            False,
            f"‚ö†Ô∏è  –ü–æ–¥–æ–∑—Ä–µ–Ω–∏–µ –Ω–∞ –ø–µ—Ä–µ—Å–ø–∞–º: '{keyword}' {count} —Ä–∞–∑ –≤ {len(words)} —Å–ª–æ–≤–∞—Ö",
        )

    return True, f"‚úÖ –ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ: {count} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π, –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–æ"


def check_nausea_metrics(md_content: str, tier: str = "B") -> Dict:
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–µ—Ç—Ä–∏–∫ —Ç–æ—à–Ω–æ—Ç—ã –∏ –≤–æ–¥—ã (SEO 2025 v7.1 –ê–¥–≤–µ–≥–æ-–∫–∞–ª–∏–±—Ä–æ–≤–∫–∞).

    Targets (v7.1):
    - Water: 40-60% (–ê–¥–≤–µ–≥–æ –Ω–æ—Ä–º–∞)
    - Classic Nausea: ‚â§3.5 (BLOCKER >4.0)
    - Academic Nausea: 7-9.5% (–ê–¥–≤–µ–≥–æ –æ–ø—Ç–∏–º—É–º)

    Args:
        md_content: Markdown –∫–æ–Ω—Ç–µ–Ω—Ç
        tier: Tier –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ—Ä–æ–≥–æ–≤ –∏–∑ seo_utils

    Returns:
        Dict with pass/message/metrics
    """
    if not NAUSEA_AVAILABLE:
        return {
            "pass": True,
            "blocker": False,
            "message": "‚ö†Ô∏è  Nausea check –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (natasha –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞)"
        }

    try:
        metrics = calculate_metrics_from_text(md_content)
    except Exception as e:
        return {
            "pass": True,
            "blocker": False,
            "message": f"‚ö†Ô∏è  Nausea check –æ—à–∏–±–∫–∞: {e}"
        }

    if not metrics:
        return {
            "pass": True,
            "blocker": False,
            "message": "‚ö†Ô∏è  Nausea check: —Ç–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π"
        }

    # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Ä–æ–≥–∏ –∏–∑ seo_utils (v7.1)
    if UTILS_AVAILABLE:
        req = get_tier_requirements(tier)
        water_min = req.get("water_min", 40.0)
        water_max = req.get("water_max", 60.0)
        nausea_classic_max = req.get("nausea_classic_max", 3.5)
        nausea_classic_blocker = req.get("nausea_classic_blocker", 4.0)
        nausea_academic_min = req.get("nausea_academic_min", 7.0)
        nausea_academic_max = req.get("nausea_academic_max", 9.5)
    else:
        # Fallback defaults (v7.1)
        water_min, water_max = 40.0, 60.0
        nausea_classic_max, nausea_classic_blocker = 3.5, 4.0
        nausea_academic_min, nausea_academic_max = 7.0, 9.5

    water = metrics['water_percent']
    classic = metrics['classic_nausea']
    academic = metrics['academic_nausea']

    issues = []
    is_blocker = False

    # Water check (40-60%)
    if water < water_min:
        issues.append(f"Water {water:.1f}% < {water_min}% (—Å—É—Ö–æ–π)")
    elif water > water_max:
        issues.append(f"Water {water:.1f}% > {water_max}%")

    # Classic Nausea check (‚â§3.5, BLOCKER >4.0)
    if classic > nausea_classic_blocker:
        issues.append(f"Classic Nausea {classic:.2f} > {nausea_classic_blocker} [BLOCKER]")
        is_blocker = True
    elif classic > nausea_classic_max:
        issues.append(f"Classic Nausea {classic:.2f} > {nausea_classic_max}")

    # Academic Nausea check (7-9.5%)
    if academic < nausea_academic_min:
        issues.append(f"Academic {academic:.1f}% < {nausea_academic_min}% (—Å—É—Ö–æ–π)")
    elif academic > nausea_academic_max:
        if academic > 12.0:
            issues.append(f"Academic {academic:.1f}% > 12% [BLOCKER]")
            is_blocker = True
        else:
            issues.append(f"Academic {academic:.1f}% > {nausea_academic_max}%")

    # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    if not issues:
        status = "‚úÖ"
        message = f"{status} Nausea/Water: Water {water:.1f}% | Classic {classic:.2f} | Academic {academic:.1f}%"
        passed = True
    else:
        status = "‚ùå" if is_blocker else "‚ö†Ô∏è"
        message = f"{status} Nausea/Water: {', '.join(issues)}"
        passed = not is_blocker

    return {
        "pass": passed,
        "blocker": is_blocker,
        "message": message,
        "metrics": {
            "water_percent": water,
            "classic_nausea": classic,
            "academic_nausea": academic,
            "most_common_lemma": metrics.get('most_common_lemma', ''),
            "max_frequency": metrics.get('max_frequency', 0)
        }
    }


def check_internal_links(md: str) -> Tuple[bool, str]:
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫ [text](url)
    """
    # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –≤–∏–¥–∞ [text](url) –∏–ª–∏ [text](https://domain/path)
    all_links = re.findall(r"\[([^\]]+)\]\(([^\)]+)\)", md)

    # –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏ –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å / –∏–ª–∏ —Å –¥–æ–º–µ–Ω–æ–º ultimate.net.ua
    internal_links = [
        link for link in all_links if link[1].startswith("/") or "ultimate.net.ua" in link[1]
    ]

    link_count = len(internal_links)

    if link_count < 2:
        return (
            True, # CHANGED to True (PASS with warning) for v7.3
            f"‚ö†Ô∏è  –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏: {link_count} (–Ω—É–∂–Ω–æ 2-5) [WARNING only]",
        )

    if link_count > 5:
        return (
            True, # CHANGED to True (PASS with warning)
            f"‚ö†Ô∏è  –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏: {link_count} (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 2-5)",
        )

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–Ω–∫–æ—Ä–æ–≤
    bad_anchors = ["–∑–¥–µ—Å—å", "—Ç—É—Ç", "—á–∏—Ç–∞—Ç—å –¥–∞–ª–µ–µ", "–ø–æ–¥—Ä–æ–±–Ω–µ–µ", "–ø–µ—Ä–µ–π—Ç–∏", "—Å—Å—ã–ª–∫–∞"]
    bad_found = []
    for anchor, _url in internal_links:
        anchor_clean = anchor.strip().lower()
        if anchor_clean in bad_anchors:
            bad_found.append(anchor)

    if bad_found:
        return (
            False,
            f"‚ö†Ô∏è  –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏: {link_count} —à—Ç, –Ω–æ –Ω–µ–æ–ø–∏—Å–∞—Ç–µ–ª—å–Ω—ã–µ –∞–Ω–∫–æ—Ä—ã: {', '.join(bad_found)}",
        )

    return True, f"‚úÖ –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏: {link_count} —à—Ç, –æ–ø–∏—Å–∞—Ç–µ–ª—å–Ω—ã–µ –∞–Ω–∫–æ—Ä—ã"


def check_content(md_file: str, keyword: str, tier: str = "B") -> Dict:
    """
    –û—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ Markdown –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    """
    try:
        metadata, md_content = parse_markdown_file(md_file)
    except FileNotFoundError:
        return {"status": "ERROR", "message": f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {md_file}"}

    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
    text = extract_text_content(md_content)
    
    if UTILS_AVAILABLE:
        word_count = count_words(text)
        words = text.split() # Keep for other checks that need list of words
    else:
        words = text.split()
        word_count = len(words)

    # –ü–æ–¥—Å—á—ë—Ç —Å–∏–º–≤–æ–ª–æ–≤ –ë–ï–ó –ø—Ä–æ–±–µ–ª–æ–≤ (RULES 2025 - BLOCKER!)
    char_count_no_spaces = count_chars_no_spaces(md_content)

    # –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –ø–æ Tier - UNIFIED —á–µ—Ä–µ–∑ seo_utils.py (RULES 2025)
    if UTILS_AVAILABLE:
        req = get_tier_requirements(tier)
    else:
        # Critical Error: seo_utils MUST be available for v7.3 validation
        print("‚ùå CRITICAL ERROR: seo_utils.py not found. Validation cannot proceed reliably.")
        return {"status": "ERROR", "message": "seo_utils.py dependency missing"}


    results = {
        "file": Path(md_file).name,
        "tier": tier,
        "word_count": word_count,
        "char_count_no_spaces": char_count_no_spaces,
        "checks": {},
        "status": "PASS",
        "metadata": metadata,
    }

    # 0. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏–º–≤–æ–ª–æ–≤ –ë–ï–ó –ø—Ä–æ–±–µ–ª–æ–≤ (Google 2025 - advisory only, not blocker)
    char_count_ok = req["char_min"] <= char_count_no_spaces <= req["char_max"]
    results["checks"]["char_count"] = {
        "pass": char_count_ok,
        "blocker": False,  # Google 2025: depth > length, char count is advisory
        "message": f"{'‚úÖ' if char_count_ok else '‚ö†Ô∏è '} –°–∏–º–≤–æ–ª–æ–≤ (–±–µ–∑ –ø—Ä–æ–±–µ–ª–æ–≤): {char_count_no_spaces} (—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: {req['char_min']}-{req['char_max']})",
    }

    # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—ä—ë–º–∞ (—Å–ª–æ–≤–∞ - –¥–ª—è —Å–ø—Ä–∞–≤–∫–∏)
    word_count_ok = req["min_words"] <= word_count <= req["max_words"]
    results["checks"]["word_count"] = {
        "pass": word_count_ok,
        "blocker": False,
        "message": f"{'‚úÖ' if word_count_ok else '‚ö†Ô∏è '} –°–ª–æ–≤: {word_count} (–æ—Ä–∏–µ–Ω—Ç–∏—Ä: {req['min_words']}-{req['max_words']})",
    }

    # 2. H1 (–≤ Markdown —ç—Ç–æ # –∑–∞–≥–æ–ª–æ–≤–æ–∫)
    h1_list = re.findall(r"^#\s+(.+)$", md_content, re.MULTILINE)
    h1_count = len(h1_list)
    h1_text = h1_list[0] if h1_list else ""
    h1_ok = h1_count == 1
    results["checks"]["h1"] = {
        "pass": h1_ok,
        "message": f"{'‚úÖ' if h1_ok else '‚ùå'} H1: {h1_count} —à—Ç (–Ω—É–∂–Ω–æ 1)",
        "text": h1_text[:60] if h1_text else "",
    }

    # 3. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∏–Ω—Ç—Ä–æ
    intro_ok, intro_msg = check_intro_structure(md_content, words)
    results["checks"]["intro"] = {"pass": intro_ok, "message": intro_msg}

    # 4. H2 —Å—Ç—Ä—É–∫—Ç—É—Ä–∞
    h2_ok, h2_msg = check_h2_intent_structure(md_content)
    results["checks"]["h2_intent"] = {"pass": h2_ok, "message": h2_msg}

    # 5. FAQ
    faq_ok, faq_msg = check_faq(md_content)
    if tier == "C":
        results["checks"]["faq"] = {"pass": True, "message": f"‚ÑπÔ∏è  {faq_msg} (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)"}
    else:
        results["checks"]["faq"] = {"pass": faq_ok, "message": faq_msg}

    # 6. Keyword stuffing (–∏—Å–ø–æ–ª—å–∑—É–µ–º md_content —Å –ø–µ—Ä–µ–Ω–æ—Å–∞–º–∏ —Å—Ç—Ä–æ–∫)
    stuffing_ok, stuffing_msg = check_keyword_stuffing(md_content, keyword)
    results["checks"]["keyword_natural"] = {"pass": stuffing_ok, "message": stuffing_msg}

    # 6.5. Keyword Density & Distribution (RULES 2025)
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç—å –∫ JSON –Ω–∞ –æ—Å–Ω–æ–≤–µ md_file
    # –ù–∞–ø—Ä–∏–º–µ—Ä: categories/aktivnaya-pena/content/aktivnaya-pena_ru.md
    #        -> categories/aktivnaya-pena/data/aktivnaya-pena.json
    md_path = Path(md_file)
    if "categories" in md_path.parts:
        # –ò–∑–≤–ª–µ–∫–∞–µ–º slug –∏–∑ –ø—É—Ç–∏
        category_idx = md_path.parts.index("categories")
        if category_idx + 1 < len(md_path.parts):
            slug = md_path.parts[category_idx + 1]

            # D+E: Fallback ‚Äî _clean.json (12 kw) ‚Üí {slug}.json (52 kw)
            clean_json_path = md_path.parent.parent / "data" / f"{slug}_clean.json"
            raw_json_path = md_path.parent.parent / "data" / f"{slug}.json"

            if clean_json_path.exists():
                data_json_path = clean_json_path
            else:
                data_json_path = raw_json_path

            if data_json_path.exists():
                density_result = check_keyword_density_and_distribution(
                    md_content, str(data_json_path), word_count, req
                )

                density = density_result["total_density"]
                coverage = density_result["coverage"]
                keywords_total = density_result.get("keywords_total", 50)

                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º severity –ø–æ Google 2025 –ø–æ—Ä–æ–≥–∞–º (people-first)
                # v7.5: –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É keywords
                # Clean JSON (‚â§20 kw): blocker = 5.0%, warning = 3.5%
                # Raw JSON (>20 kw): blocker = 3.5%, warning = 2.5%
                if keywords_total <= 20:
                    density_blocker = 5.0
                    density_warning = 3.5
                else:
                    density_blocker = 3.5
                    density_warning = 2.5

                coverage_min = req.get("coverage", 0.30) * 100  # –°–Ω–∏–∂–µ–Ω–æ –¥–ª—è LMF
                if density > density_blocker:
                    severity = "FAIL"
                elif density > density_warning or coverage < coverage_min:
                    severity = "REVIEW"
                else:
                    severity = "PASS"

                density_pass = severity != "FAIL"

                status_icon = (
                    "‚úÖ" if severity == "PASS"
                    else "‚ö†Ô∏è" if severity == "REVIEW"
                    else "‚ùå"
                )

                status_msg = (
                    f"{status_icon} Density: {density:.2f}% | "
                    f"Coverage: {coverage:.0f}%"
                )

                if density_result.get("warnings"):
                    status_msg += f" | {len(density_result['warnings'])} –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π"

                if density_result.get("errors"):
                    status_msg += f" | {len(density_result['errors'])} –æ—à–∏–±–æ–∫"

                results["checks"]["density_distribution"] = {
                    "pass": density_pass,
                    "blocker": True,
                    "severity": severity,
                    "message": status_msg,
                    "details": density_result
                }
            else:
                results["checks"]["density_distribution"] = {
                    "pass": True,
                    "blocker": False,
                    "message": f"‚ö†Ô∏è  JSON –Ω–µ –Ω–∞–π–¥–µ–Ω: {data_json_path.name}"
                }
    else:
        results["checks"]["density_distribution"] = {
            "pass": True,
            "blocker": False,
            "message": "‚ö†Ô∏è  –ü—É—Ç—å –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç 'categories/', –ø—Ä–æ–ø—É—Å–∫ –ø—Ä–æ–≤–µ—Ä–∫–∏ density"
        }

    # 7. –ü–µ—Ä–≤—ã–µ 100 —Å–ª–æ–≤
    first_100 = " ".join(words[:100])
    keyword_in_first_100 = keyword.lower() in first_100.lower()
    results["checks"]["first_100"] = {
        "pass": keyword_in_first_100,
        "message": f"{'‚úÖ' if keyword_in_first_100 else '‚ö†Ô∏è '} –ü–µ—Ä–≤—ã–µ 100 —Å–ª–æ–≤: –∫–ª—é—á {'–Ω–∞–π–¥–µ–Ω' if keyword_in_first_100 else '–ù–ï –ù–ê–ô–î–ï–ù'}",
    }

    # 8. –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏
    links_ok, links_msg = check_internal_links(md_content)
    results["checks"]["internal_links"] = {"pass": links_ok, "message": links_msg}

    # 8.5. Nausea/Water check (SEO 2025 v7.1 –ê–¥–≤–µ–≥–æ-–∫–∞–ª–∏–±—Ä–æ–≤–∫–∞)
    nausea_result = check_nausea_metrics(md_content, tier)
    results["checks"]["nausea_water"] = nausea_result

    # 9. Title (–∏–∑ YAML)
    title_text = metadata.get("title", "")
    title_len = len(title_text)
    title_ok = 50 <= title_len <= 70
    results["checks"]["title"] = {
        "pass": title_ok,
        "message": f"{'‚úÖ' if title_ok else '‚ö†Ô∏è '} Title: {title_len} —Å–∏–º–≤–æ–ª–æ–≤ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 50-70)",
        "text": title_text,
    }

    # 10. Description (–∏–∑ YAML)
    desc_text = metadata.get("description", "")
    desc_len = len(desc_text)
    desc_ok = 140 <= desc_len <= 170
    results["checks"]["description"] = {
        "pass": desc_ok,
        "message": f"{'‚úÖ' if desc_ok else '‚ö†Ô∏è '} Description: {desc_len} —Å–∏–º–≤–æ–ª–æ–≤ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 140-170)",
        "text": desc_text[:80] if desc_text else "",
    }

    # 11. Schema.org (–¥–ª—è MD –ø—Ä–æ–ø—É—Å–∫–∞–µ–º, —Ç.–∫. –±—É–¥–µ—Ç –≤ –ë–î)
    results["checks"]["schema"] = {
        "pass": True,
        "message": "‚ÑπÔ∏è  Schema.org: –±—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω–æ –ø—Ä–∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤ OpenCart",
    }

    # –ò—Ç–æ–≥–æ–≤—ã–π —Å—Ç–∞—Ç—É—Å
    # BLOCKER checks - Google 2025 (char_count removed - depth > length)
    blocker_checks = ["nausea_water"]  # char_count —Ç–µ–ø–µ—Ä—å advisory, density –≤—ã–Ω–µ—Å–µ–Ω –≤ severity
    # Critical checks
    critical_checks = ["h1", "intro", "h2_intent", "keyword_natural", "internal_links"]

    all_blocker_pass = all(
        results["checks"][check]["pass"]
        for check in blocker_checks
        if check in results["checks"]
    )
    all_critical_pass = all(
        results["checks"][check]["pass"]
        for check in critical_checks
        if check in results["checks"]
    )

    density_severity = results["checks"].get(
        "density_distribution", {}
    ).get("severity", "PASS")

    if not all_blocker_pass or not all_critical_pass or density_severity == "FAIL":
        results["status"] = "FAIL"
    else:
        optional_checks = ["word_count", "first_100"]
        optional_pass = sum(
            1
            for check in optional_checks
            if check in results["checks"] and results["checks"][check]["pass"]
        )

        if density_severity == "REVIEW" or optional_pass < len(optional_checks) * 0.7:
            results["status"] = "REVIEW"
        else:
            results["status"] = "PASS"

    return results


def print_report(results: Dict):
    """–í—ã–≤–æ–¥ –æ—Ç—á—ë—Ç–∞"""
    print(f"\n{'=' * 70}")
    print(f"üìÑ –ü–†–û–í–ï–†–ö–ê: {results['file']}")
    print(f"üéØ TIER: {results['tier']}")
    print(f"üìä –°–ª–æ–≤: {results['word_count']} | –°–∏–º–≤–æ–ª–æ–≤ (–±–µ–∑ –ø—Ä–æ–±–µ–ª–æ–≤): {results.get('char_count_no_spaces', 'N/A')}")
    print(f"{'=' * 70}\n")

    for check_name, check_data in results["checks"].items():
        print(check_data["message"])
        if "text" in check_data and check_data["text"]:
            print(f'   ‚îî‚îÄ "{check_data["text"]}..."')

        # –ï—Å–ª–∏ —ç—Ç–æ density check, –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—é
        if check_name == "density_distribution" and "details" in check_data:
            density_details = check_data["details"]
            if density_details.get("details"):
                print(f"\n   üìä –î–ï–¢–ê–õ–ò–ó–ê–¶–ò–Ø KEYWORDS (Top 10):")
                print(f"   {'Keyword':<40} {'Type':<10} {'Exact':<6} {'Partial':<7} {'Total':<6} {'Density':<8} {'Target'}")
                print(f"   {'-' * 100}")

                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10 keywords
                for i, kw in enumerate(density_details["details"][:10]):
                    status = kw["status"]
                    print(f"   {status} {kw['keyword']:<38} {kw['type']:<10} {kw['exact']:<6} {kw['partial']:<7} {kw['total']:<6} {kw['density_actual']:<8} {kw['density_target']}")

                total_kw = len(density_details["details"])
                if total_kw > 10:
                    print(f"   ... –∏ –µ—â—ë {total_kw - 10} keywords")
                print()

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º warnings
            if density_details.get("warnings"):
                print(f"\n   ‚ö†Ô∏è  –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–Ø ({len(density_details['warnings'])}):")
                for warning in density_details["warnings"][:5]:  # –ü–µ—Ä–≤—ã–µ 5
                    print(f"      ‚Ä¢ {warning}")
                if len(density_details['warnings']) > 5:
                    print(f"      ... –∏ –µ—â—ë {len(density_details['warnings']) - 5} –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π")
                print()

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º errors
            if density_details.get("errors"):
                print(f"\n   ‚ùå –û–®–ò–ë–ö–ò ({len(density_details['errors'])}):")
                for error in density_details["errors"]:
                    print(f"      ‚Ä¢ {error}")
                print()

    print(f"\n{'=' * 70}")
    status_icons = {"PASS": "‚úÖ", "REVIEW": "‚ö†Ô∏è ", "FAIL": "‚ùå"}
    print(f"{status_icons[results['status']]} –†–ï–ó–£–õ–¨–¢–ê–¢: {results['status']}")

    if results["status"] == "PASS":
        print("   –ö–æ–Ω—Ç–µ–Ω—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç Google Search Essentials 2025")
    elif results["status"] == "REVIEW":
        print("   –ö–æ–Ω—Ç–µ–Ω—Ç —Ç—Ä–µ–±—É–µ—Ç –¥–æ—Ä–∞–±–æ—Ç–∫–∏ optional –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
    else:
        print("   –ö–æ–Ω—Ç–µ–Ω—Ç —Ç—Ä–µ–±—É–µ—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π –ø–µ—Ä–µ–¥ –ø—É–±–ª–∏–∫–∞—Ü–∏–µ–π")

    print(f"{'=' * 70}\n")


def save_json_report(results: Dict, md_file: str):
    """
    –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ JSON –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ —á—Ç–µ–Ω–∏—è

    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤: <md_file>_validation.json

    Args:
        results: —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã check_content()
        md_file: –ø—É—Ç—å –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É MD —Ñ–∞–π–ª—É
    """
    md_path = Path(md_file)
    json_path = md_path.parent / f"{md_path.stem}_validation.json"

    # –£–ø—Ä–æ—â–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è JSON (—É–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è, –æ—Å—Ç–∞–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏)
    json_output = {
        "file": results["file"],
        "tier": results["tier"],
        "word_count": results["word_count"],
        "char_count_no_spaces": results.get("char_count_no_spaces", 0),
        "status": results["status"],
        "checks": {}
    }

    # –£–ø—Ä–æ—â–∞–µ–º checks –¥–ª—è JSON
    for check_name, check_data in results["checks"].items():
        json_output["checks"][check_name] = {
            "pass": check_data["pass"],
            "blocker": check_data.get("blocker", False)
        }

        # –î–æ–±–∞–≤–ª—è–µ–º density –¥–µ—Ç–∞–ª–∏ –µ—Å–ª–∏ –µ—Å—Ç—å
        if check_name == "density_distribution" and "details" in check_data:
            density_details = check_data["details"]
            json_output["checks"][check_name]["metrics"] = {
                "total_density": density_details.get("total_density", 0.0),
                "coverage": density_details.get("coverage", 0.0),
                "keywords_found": density_details.get("keywords_found", 0),
                "keywords_total": density_details.get("keywords_total", 0),
                "warnings_count": len(density_details.get("warnings", [])),
                "errors_count": len(density_details.get("errors", [])),
                "details": density_details.get("details", [])
            }

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(json_output, f, ensure_ascii=False, indent=2)

    print(f"\n‚úÖ JSON report —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {json_path}")
    return str(json_path)


def main():
    import argparse

    parser = argparse.ArgumentParser(
        description="SEO Validator v2.0 MD - Google 2025 Compatible",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã:
  python check_simple_v2_md.py content.md "–∞–∫—Ç–∏–≤–Ω–∞—è –ø–µ–Ω–∞" A
  python check_simple_v2_md.py content.md "–∞–∫—Ç–∏–≤–Ω–∞—è –ø–µ–Ω–∞" B --json

Tier: A (–ø—Ä–µ–º–∏—É–º) / B (—Å—Ç–∞–Ω–¥–∞—Ä—Ç) / C (–º–∏–Ω–∏–º—É–º)
        """
    )

    parser.add_argument("md_file", help="–ü—É—Ç—å –∫ Markdown —Ñ–∞–π–ª—É")
    parser.add_argument("keyword", help="–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏")
    parser.add_argument("tier", nargs='?', default="B", help="Tier –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (A/B/C)")
    parser.add_argument("--json", action="store_true", help="–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ JSON")

    args = parser.parse_args()

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞
    if not Path(args.md_file).exists():
        print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {args.md_file}")
        sys.exit(1)

    tier = args.tier.upper()
    results = check_content(args.md_file, args.keyword, tier)

    # –¢–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á—ë—Ç (–≤—Å–µ–≥–¥–∞)
    print_report(results)

    # JSON –æ—Ç—á—ë—Ç (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    if args.json:
        save_json_report(results, args.md_file)

    if results["status"] == "PASS":
        code = 0
    elif results["status"] == "REVIEW":
        code = 1
    else:
        code = 2

    sys.exit(code)


if __name__ == "__main__":
    main()
